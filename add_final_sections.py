#!/usr/bin/env python3
"""
Script to add final sections: Grad-CAM, ONNX export, and deployment
"""

import json

def create_cell(cell_type, source, metadata=None):
    """Create a notebook cell"""
    cell = {
        "cell_type": cell_type,
        "metadata": metadata or {},
        "source": source if isinstance(source, list) else [source]
    }
    
    if cell_type == "code":
        cell["execution_count"] = None
        cell["outputs"] = []
    
    return cell

# Load existing notebook
with open('/workspace/diabetic_retinopathy_detection.ipynb', 'r') as f:
    notebook = json.load(f)

# Final sections
final_cells = []

# Grad-CAM section
final_cells.append(create_cell("markdown", [
    "## 4Ô∏è‚É£ Explainability with Grad-CAM\n",
    "\n",
    "Let's use Grad-CAM (Gradient-weighted Class Activation Mapping) to visualize which parts of the retinal images our model focuses on when making predictions."
]))

final_cells.append(create_cell("code", [
    "# Grad-CAM implementation\n",
    "def get_gradcam_visualization(model, image, target_class, device):\n",
    "    \"\"\"Generate Grad-CAM visualization for a given image and target class\"\"\"\n",
    "    \n",
    "    # Define target layer (last convolutional layer of ResNet50)\n",
    "    target_layers = [model.backbone.layer4[-1]]\n",
    "    \n",
    "    # Initialize Grad-CAM\n",
    "    cam = GradCAM(model=model, target_layers=target_layers)\n",
    "    \n",
    "    # Generate CAM\n",
    "    targets = [ClassifierOutputTarget(target_class)]\n",
    "    \n",
    "    # Get gradcam output\n",
    "    grayscale_cam = cam(input_tensor=image.unsqueeze(0), targets=targets)\n",
    "    grayscale_cam = grayscale_cam[0, :]  # Remove batch dimension\n",
    "    \n",
    "    return grayscale_cam\n",
    "\n",
    "def visualize_gradcam_samples(model, dataset, device, num_samples=10):\n",
    "    \"\"\"Visualize Grad-CAM for sample images from each class\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Get samples from each class\n",
    "    samples_per_class = max(1, num_samples // len(class_names))\n",
    "    \n",
    "    fig, axes = plt.subplots(len(class_names), 3, figsize=(15, 3*len(class_names)))\n",
    "    fig.suptitle('Grad-CAM Visualizations by Class', fontsize=16)\n",
    "    \n",
    "    for class_id in range(len(class_names)):\n",
    "        # Find samples from this class\n",
    "        class_indices = [i for i, (_, label) in enumerate(dataset) if label == class_id]\n",
    "        \n",
    "        if len(class_indices) == 0:\n",
    "            # No samples for this class\n",
    "            for j in range(3):\n",
    "                axes[class_id, j].text(0.5, 0.5, f'No {class_names[class_id]} samples', \n",
    "                                     ha='center', va='center', transform=axes[class_id, j].transAxes)\n",
    "                axes[class_id, j].axis('off')\n",
    "            continue\n",
    "        \n",
    "        # Get a random sample\n",
    "        sample_idx = np.random.choice(class_indices)\n",
    "        image, label = dataset[sample_idx]\n",
    "        \n",
    "        # Move to device\n",
    "        image_tensor = image.to(device)\n",
    "        \n",
    "        # Get model prediction\n",
    "        with torch.no_grad():\n",
    "            output = model(image_tensor.unsqueeze(0))\n",
    "            probabilities = torch.softmax(output, dim=1)\n",
    "            predicted_class = torch.argmax(output, dim=1).item()\n",
    "            confidence = probabilities[0, predicted_class].item()\n",
    "        \n",
    "        try:\n",
    "            # Generate Grad-CAM\n",
    "            gradcam = get_gradcam_visualization(model, image_tensor, predicted_class, device)\n",
    "            \n",
    "            # Convert image to numpy for visualization\n",
    "            # Denormalize image\n",
    "            mean = np.array([0.485, 0.456, 0.406])\n",
    "            std = np.array([0.229, 0.224, 0.225])\n",
    "            \n",
    "            img_np = image.cpu().numpy().transpose(1, 2, 0)\n",
    "            img_np = std * img_np + mean\n",
    "            img_np = np.clip(img_np, 0, 1)\n",
    "            \n",
    "            # Create overlay\n",
    "            visualization = show_cam_on_image(img_np, gradcam, use_rgb=True)\n",
    "            \n",
    "            # Plot original image\n",
    "            axes[class_id, 0].imshow(img_np)\n",
    "            axes[class_id, 0].set_title(f'{class_names[class_id]}\\nOriginal')\n",
    "            axes[class_id, 0].axis('off')\n",
    "            \n",
    "            # Plot Grad-CAM heatmap\n",
    "            axes[class_id, 1].imshow(gradcam, cmap='jet')\n",
    "            axes[class_id, 1].set_title('Grad-CAM Heatmap')\n",
    "            axes[class_id, 1].axis('off')\n",
    "            \n",
    "            # Plot overlay\n",
    "            axes[class_id, 2].imshow(visualization)\n",
    "            axes[class_id, 2].set_title(f'Overlay\\nPred: {class_names[predicted_class]} ({confidence:.2f})')\n",
    "            axes[class_id, 2].axis('off')\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error generating Grad-CAM for class {class_names[class_id]}: {e}\")\n",
    "            # Show error message\n",
    "            for j in range(3):\n",
    "                axes[class_id, j].text(0.5, 0.5, f'Grad-CAM Error\\n{class_names[class_id]}', \n",
    "                                     ha='center', va='center', transform=axes[class_id, j].transAxes)\n",
    "                axes[class_id, j].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('./outputs/gradcam_visualizations.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "# Generate Grad-CAM visualizations\n",
    "print(\"Generating Grad-CAM visualizations...\")\n",
    "try:\n",
    "    visualize_gradcam_samples(model, test_dataset, device, num_samples=len(class_names))\n",
    "    print(\"‚úÖ Grad-CAM visualizations generated successfully!\")\nexcept Exception as e:\n",
    "    print(f\"‚ùå Error generating Grad-CAM visualizations: {e}\")\n",
    "    print(\"This might be due to missing images or model issues.\")"
]))

# ONNX Export section
final_cells.append(create_cell("markdown", [
    "## 5Ô∏è‚É£ Model Export for Next.js Integration\n",
    "\n",
    "We'll export our trained model to ONNX format for use in a Next.js application with `onnxruntime-node`."
]))

final_cells.append(create_cell("code", [
    "# Export model to ONNX format\n",
    "def export_to_onnx(model, save_path, input_size=(1, 3, 224, 224)):\n",
    "    \"\"\"Export PyTorch model to ONNX format\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Create dummy input\n",
    "    dummy_input = torch.randn(input_size).to(device)\n",
    "    \n",
    "    # Export to ONNX\n",
    "    torch.onnx.export(\n",
    "        model,\n",
    "        dummy_input,\n",
    "        save_path,\n",
    "        export_params=True,\n",
    "        opset_version=11,\n",
    "        do_constant_folding=True,\n",
    "        input_names=['input'],\n",
    "        output_names=['output'],\n",
    "        dynamic_axes={\n",
    "            'input': {0: 'batch_size'},\n",
    "            'output': {0: 'batch_size'}\n",
    "        }\n",
    "    )\n",
    "\n",
    "def verify_onnx_model(onnx_path, pytorch_model, device):\n",
    "    \"\"\"Verify ONNX model produces same results as PyTorch model\"\"\"\n",
    "    # Load ONNX model\n",
    "    ort_session = ort.InferenceSession(onnx_path)\n",
    "    \n",
    "    # Create test input\n",
    "    test_input = torch.randn(1, 3, 224, 224).to(device)\n",
    "    \n",
    "    # PyTorch prediction\n",
    "    pytorch_model.eval()\n",
    "    with torch.no_grad():\n",
    "        pytorch_output = pytorch_model(test_input).cpu().numpy()\n",
    "    \n",
    "    # ONNX prediction\n",
    "    ort_inputs = {ort_session.get_inputs()[0].name: test_input.cpu().numpy()}\n",
    "    ort_output = ort_session.run(None, ort_inputs)[0]\n",
    "    \n",
    "    # Compare outputs\n",
    "    max_diff = np.max(np.abs(pytorch_output - ort_output))\n",
    "    \n",
    "    return max_diff < 1e-5, max_diff\n",
    "\n",
    "# Export model\n",
    "print(\"Exporting model to ONNX format...\")\n",
    "onnx_path = './outputs/diabetic_retinopathy_model.onnx'\n",
    "\n",
    "try:\n",
    "    export_to_onnx(model, onnx_path)\n",
    "    print(f\"‚úÖ Model exported to: {onnx_path}\")\n",
    "    \n",
    "    # Verify ONNX model\n",
    "    is_valid, max_diff = verify_onnx_model(onnx_path, model, device)\n",
    "    \n",
    "    if is_valid:\n",
    "        print(f\"‚úÖ ONNX model verification successful (max diff: {max_diff:.2e})\")\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è  ONNX model verification failed (max diff: {max_diff:.2e})\")\n",
    "    \n",
    "    # Get model size\n",
    "    import os\n",
    "    model_size_mb = os.path.getsize(onnx_path) / (1024 * 1024)\n",
    "    print(f\"Model size: {model_size_mb:.2f} MB\")\n",
    "    \nexcept Exception as e:\n",
    "    print(f\"‚ùå Error exporting to ONNX: {e}\")"
]))

# Preprocessing parameters export
final_cells.append(create_cell("code", [
    "# Export preprocessing parameters for frontend\n",
    "preprocessing_config = {\n",
    "    'image_size': CONFIG['IMAGE_SIZE'],\n",
    "    'mean': [0.485, 0.456, 0.406],\n",
    "    'std': [0.229, 0.224, 0.225],\n",
    "    'class_names': class_names,\n",
    "    'num_classes': CONFIG['NUM_CLASSES'],\n",
    "    'model_architecture': CONFIG['MODEL_NAME'],\n",
    "    'input_shape': [1, 3, CONFIG['IMAGE_SIZE'], CONFIG['IMAGE_SIZE']],\n",
    "    'output_shape': [1, CONFIG['NUM_CLASSES']]\n",
    "}\n",
    "\n",
    "# Save preprocessing config\n",
    "with open('./outputs/preprocessing_config.json', 'w') as f:\n",
    "    json.dump(preprocessing_config, f, indent=2)\n",
    "\n",
    "print(\"Preprocessing configuration saved to: ./outputs/preprocessing_config.json\")\n",
    "print(\"\\nPreprocessing Config:\")\n",
    "for key, value in preprocessing_config.items():\n",
    "    print(f\"{key}: {value}\")"
]))

# Optional quantization
final_cells.append(create_cell("code", [
    "# Optional: Quantize ONNX model to reduce size\n",
    "def quantize_onnx_model(input_path, output_path):\n",
    "    \"\"\"Quantize ONNX model to reduce size\"\"\"\n",
    "    try:\n",
    "        from onnxruntime.quantization import quantize_dynamic, QuantType\n",
    "        \n",
    "        quantize_dynamic(\n",
    "            input_path,\n",
    "            output_path,\n",
    "            weight_type=QuantType.QUInt8\n",
    "        )\n",
    "        \n",
    "        return True\n",
    "    except ImportError:\n",
    "        print(\"ONNX quantization not available. Install with: pip install onnxruntime-tools\")\n",
    "        return False\n",
    "    except Exception as e:\n",
    "        print(f\"Quantization error: {e}\")\n",
    "        return False\n",
    "\n",
    "# Quantize model\n",
    "print(\"\\nAttempting to quantize ONNX model...\")\n",
    "quantized_path = './outputs/diabetic_retinopathy_model_quantized.onnx'\n",
    "\n",
    "if os.path.exists(onnx_path):\n",
    "    success = quantize_onnx_model(onnx_path, quantized_path)\n",
    "    \n",
    "    if success and os.path.exists(quantized_path):\n",
    "        original_size = os.path.getsize(onnx_path) / (1024 * 1024)\n",
    "        quantized_size = os.path.getsize(quantized_path) / (1024 * 1024)\n",
    "        compression_ratio = original_size / quantized_size\n",
    "        \n",
    "        print(f\"‚úÖ Quantized model saved to: {quantized_path}\")\n",
    "        print(f\"Original size: {original_size:.2f} MB\")\n",
    "        print(f\"Quantized size: {quantized_size:.2f} MB\")\n",
    "        print(f\"Compression ratio: {compression_ratio:.2f}x\")\n",
    "    else:\n",
    "        print(\"‚ùå Quantization failed or not available\")\nelse:\n",
    "    print(\"No ONNX model found for quantization\")"
]))

# Next.js integration guide
final_cells.append(create_cell("markdown", [
    "## 7Ô∏è‚É£ Next.js Integration Guide\n",
    "\n",
    "Here's how to integrate the exported ONNX model into your Next.js application:"
]))

final_cells.append(create_cell("code", [
    "# Generate Next.js integration code\n",
    "nextjs_integration_code = '''\n",
    "// Next.js API Route Example: /pages/api/predict.js or /app/api/predict/route.js\n",
    "\n",
    "import * as ort from 'onnxruntime-node';\n",
    "import sharp from 'sharp';\n",
    "import path from 'path';\n",
    "import fs from 'fs';\n",
    "\n",
    "// Load preprocessing config\n",
    "const configPath = path.join(process.cwd(), 'models', 'preprocessing_config.json');\n",
    "const config = JSON.parse(fs.readFileSync(configPath, 'utf8'));\n",
    "\n",
    "// Load ONNX model\n",
    "let session = null;\n",
    "\n",
    "async function loadModel() {\n",
    "  if (!session) {\n",
    "    const modelPath = path.join(process.cwd(), 'models', 'diabetic_retinopathy_model.onnx');\n",
    "    session = await ort.InferenceSession.create(modelPath);\n",
    "  }\n",
    "  return session;\n",
    "}\n",
    "\n",
    "// Preprocess image\n",
    "async function preprocessImage(imageBuffer) {\n",
    "  // Resize and normalize image\n",
    "  const { data, info } = await sharp(imageBuffer)\n",
    "    .resize(config.image_size, config.image_size)\n",
    "    .raw()\n",
    "    .toBuffer({ resolveWithObject: true });\n",
    "  \n",
    "  // Convert to float32 and normalize\n",
    "  const pixels = new Float32Array(data.length);\n",
    "  \n",
    "  for (let i = 0; i < data.length; i += 3) {\n",
    "    // Normalize RGB channels\n",
    "    pixels[i] = (data[i] / 255.0 - config.mean[0]) / config.std[0];     // R\n",
    "    pixels[i + 1] = (data[i + 1] / 255.0 - config.mean[1]) / config.std[1]; // G\n",
    "    pixels[i + 2] = (data[i + 2] / 255.0 - config.mean[2]) / config.std[2]; // B\n",
    "  }\n",
    "  \n",
    "  // Reshape to [1, 3, 224, 224] format\n",
    "  const tensor = new Float32Array(1 * 3 * config.image_size * config.image_size);\n",
    "  \n",
    "  for (let c = 0; c < 3; c++) {\n",
    "    for (let h = 0; h < config.image_size; h++) {\n",
    "      for (let w = 0; w < config.image_size; w++) {\n",
    "        const pixelIndex = (h * config.image_size + w) * 3 + c;\n",
    "        const tensorIndex = c * config.image_size * config.image_size + h * config.image_size + w;\n",
    "        tensor[tensorIndex] = pixels[pixelIndex];\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "  \n",
    "  return tensor;\n",
    "}\n",
    "\n",
    "// API handler\n",
    "export default async function handler(req, res) {\n",
    "  if (req.method !== 'POST') {\n",
    "    return res.status(405).json({ error: 'Method not allowed' });\n",
    "  }\n",
    "  \n",
    "  try {\n",
    "    // Get image from request\n",
    "    const { image } = req.body; // Base64 encoded image\n",
    "    const imageBuffer = Buffer.from(image, 'base64');\n",
    "    \n",
    "    // Preprocess image\n",
    "    const inputTensor = await preprocessImage(imageBuffer);\n",
    "    \n",
    "    // Load model and run inference\n",
    "    const session = await loadModel();\n",
    "    const feeds = { input: new ort.Tensor('float32', inputTensor, [1, 3, 224, 224]) };\n",
    "    const results = await session.run(feeds);\n",
    "    \n",
    "    // Get predictions\n",
    "    const output = results.output.data;\n",
    "    \n",
    "    // Apply softmax to get probabilities\n",
    "    const maxLogit = Math.max(...output);\n",
    "    const expLogits = output.map(x => Math.exp(x - maxLogit));\n",
    "    const sumExp = expLogits.reduce((a, b) => a + b, 0);\n",
    "    const probabilities = expLogits.map(x => x / sumExp);\n",
    "    \n",
    "    // Get predicted class\n",
    "    const predictedClass = probabilities.indexOf(Math.max(...probabilities));\n",
    "    const confidence = probabilities[predictedClass];\n",
    "    \n",
    "    // Prepare response\n",
    "    const response = {\n",
    "      predicted_class: predictedClass,\n",
    "      predicted_label: config.class_names[predictedClass],\n",
    "      confidence: confidence,\n",
    "      probabilities: probabilities.reduce((acc, prob, idx) => {\n",
    "        acc[config.class_names[idx]] = prob;\n",
    "        return acc;\n",
    "      }, {})\n",
    "    };\n",
    "    \n",
    "    res.status(200).json(response);\n",
    "    \n",
    "  } catch (error) {\n",
    "    console.error('Prediction error:', error);\n",
    "    res.status(500).json({ error: 'Prediction failed', details: error.message });\n",
    "  }\n",
    "}\n",
    "'''\n",
    "\n",
    "# Save Next.js integration code\n",
    "with open('./outputs/nextjs_integration.js', 'w') as f:\n",
    "    f.write(nextjs_integration_code.strip())\n",
    "\n",
    "print(\"Next.js integration code saved to: ./outputs/nextjs_integration.js\")\n",
    "print(\"\\nIntegration Steps:\")\n",
    "print(\"1. Copy the ONNX model and preprocessing config to your Next.js project\")\n",
    "print(\"2. Install dependencies: npm install onnxruntime-node sharp\")\n",
    "print(\"3. Create the API route using the provided code\")\n",
    "print(\"4. Test the API with retinal images\")"
]))

# Model performance summary
final_cells.append(create_cell("markdown", [
    "## üìä Final Model Performance Summary\n",
    "\n",
    "Let's summarize the final model performance and provide recommendations."
]))

final_cells.append(create_cell("code", [
    "# Create comprehensive model report\n",
    "def create_model_report():\n",
    "    \"\"\"Create a comprehensive model performance report\"\"\"\n",
    "    \n",
    "    report = {\n",
    "        'model_info': {\n",
    "            'architecture': CONFIG['MODEL_NAME'],\n",
    "            'num_classes': CONFIG['NUM_CLASSES'],\n",
    "            'input_size': f\"{CONFIG['IMAGE_SIZE']}x{CONFIG['IMAGE_SIZE']}\",\n",
    "            'total_parameters': sum(p.numel() for p in model.parameters()),\n",
    "            'trainable_parameters': model.get_trainable_params()\n",
    "        },\n",
    "        'training_info': {\n",
    "            'total_epochs': len(history['train_loss']) if history['train_loss'] else 0,\n",
    "            'phase1_epochs': phase1_epochs if 'phase1_epochs' in locals() else 0,\n",
    "            'phase2_epochs': phase2_epochs if 'phase2_epochs' in locals() else 0,\n",
    "            'batch_size': CONFIG['BATCH_SIZE'],\n",
    "            'learning_rate': CONFIG['LEARNING_RATE']\n",
    "        },\n",
    "        'dataset_info': {\n",
    "            'total_samples': len(labels_final) if 'labels_final' in locals() else 0,\n",
    "            'train_samples': len(train_df) if 'train_df' in locals() else 0,\n",
    "            'val_samples': len(val_df) if 'val_df' in locals() else 0,\n",
    "            'test_samples': len(test_df) if 'test_df' in locals() else 0,\n",
    "            'class_distribution': dict(labels_final['label_clean'].value_counts().sort_index()) if 'labels_final' in locals() else {}\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Add performance metrics if available\n",
    "    if 'test_metrics' in locals() and test_metrics:\n",
    "        report['performance'] = {\n",
    "            'test_accuracy': test_metrics['accuracy'],\n",
    "            'test_precision': test_metrics['precision'],\n",
    "            'test_recall': test_metrics['recall'],\n",
    "            'test_f1': test_metrics['f1'],\n",
    "            'test_auc': test_metrics.get('auc', 0.0)\n",
    "        }\n",
    "    \n",
    "    # Add training history if available\n",
    "    if history['train_loss']:\n",
    "        report['training_history'] = {\n",
    "            'best_val_loss': min(history['val_loss']),\n",
    "            'best_val_accuracy': max(history['val_acc']),\n",
    "            'best_val_f1': max(history['val_f1']),\n",
    "            'best_val_auc': max(history['val_auc'])\n",
    "        }\n",
    "    \n",
    "    return report\n",
    "\n",
    "# Generate report\n",
    "model_report = create_model_report()\n",
    "\n",
    "# Save report\n",
    "with open('./outputs/model_report.json', 'w') as f:\n",
    "    json.dump(model_report, f, indent=2)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"DIABETIC RETINOPATHY DETECTION MODEL REPORT\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\\nüìã Model Information:\")\n",
    "for key, value in model_report['model_info'].items():\n",
    "    print(f\"  {key.replace('_', ' ').title()}: {value:,}\" if isinstance(value, int) else f\"  {key.replace('_', ' ').title()}: {value}\")\n",
    "\n",
    "print(f\"\\nüéØ Training Information:\")\n",
    "for key, value in model_report['training_info'].items():\n",
    "    print(f\"  {key.replace('_', ' ').title()}: {value}\")\n",
    "\n",
    "print(f\"\\nüìä Dataset Information:\")\n",
    "for key, value in model_report['dataset_info'].items():\n",
    "    if key != 'class_distribution':\n",
    "        print(f\"  {key.replace('_', ' ').title()}: {value}\")\n",
    "\n",
    "if 'class_distribution' in model_report['dataset_info'] and model_report['dataset_info']['class_distribution']:\n",
    "    print(f\"\\n  Class Distribution:\")\n",
    "    for class_id, count in model_report['dataset_info']['class_distribution'].items():\n",
    "        if class_id < len(class_names):\n",
    "            print(f\"    {class_names[class_id]}: {count}\")\n",
    "\n",
    "if 'performance' in model_report:\n",
    "    print(f\"\\nüèÜ Test Performance:\")\n",
    "    for key, value in model_report['performance'].items():\n",
    "        print(f\"  {key.replace('_', ' ').title()}: {value:.4f}\")\n",
    "\n",
    "if 'training_history' in model_report:\n",
    "    print(f\"\\nüìà Best Training Metrics:\")\n",
    "    for key, value in model_report['training_history'].items():\n",
    "        print(f\"  {key.replace('_', ' ').title()}: {value:.4f}\")\n",
    "\n",
    "print(f\"\\nüíæ Exported Files:\")\n",
    "exported_files = [\n",
    "    './outputs/diabetic_retinopathy_model.onnx',\n",
    "    './outputs/preprocessing_config.json',\n",
    "    './outputs/nextjs_integration.js',\n",
    "    './outputs/model_report.json'\n",
    "]\n",
    "\n",
    "for file_path in exported_files:\n",
    "    if os.path.exists(file_path):\n",
    "        file_size = os.path.getsize(file_path) / 1024  # KB\n",
    "        print(f\"  ‚úÖ {file_path} ({file_size:.1f} KB)\")\n",
    "    else:\n",
    "        print(f\"  ‚ùå {file_path} (not found)\")\n",
    "\n",
    "print(f\"\\nüöÄ Ready for deployment!\")\n",
    "print(\"Model report saved to: ./outputs/model_report.json\")"
]))

# Deployment recommendations
final_cells.append(create_cell("markdown", [
    "## üöÄ Deployment Recommendations\n",
    "\n",
    "### Model Performance Considerations\n",
    "\n",
    "1. **Sensitivity vs Specificity**: For medical applications, consider the trade-off between false positives and false negatives\n",
    "2. **Confidence Thresholds**: Implement confidence-based decision making\n",
    "3. **Edge Cases**: Handle low-quality images and edge cases gracefully\n",
    "\n",
    "### Production Deployment\n",
    "\n",
    "1. **Model Serving**: Use the exported ONNX model with onnxruntime-node\n",
    "2. **Image Processing**: Ensure consistent preprocessing pipeline\n",
    "3. **Error Handling**: Implement robust error handling for various image formats\n",
    "4. **Monitoring**: Track prediction confidence and model performance over time\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "1. **Clinical Validation**: Validate model performance with medical professionals\n",
    "2. **Regulatory Compliance**: Ensure compliance with medical device regulations\n",
    "3. **Continuous Learning**: Implement feedback loops for model improvement\n",
    "4. **A/B Testing**: Compare model versions in production\n",
    "\n",
    "### Files Generated\n",
    "\n",
    "- `diabetic_retinopathy_model.onnx`: Main model for inference\n",
    "- `preprocessing_config.json`: Preprocessing parameters\n",
    "- `nextjs_integration.js`: Next.js API route example\n",
    "- `model_report.json`: Comprehensive model report\n",
    "- Various visualization images (training history, confusion matrix, ROC curves, Grad-CAM)\n",
    "\n",
    "**üéâ Congratulations! Your diabetic retinopathy detection pipeline is complete and ready for integration!**"
]))

# Add the final cells to the notebook
notebook['cells'].extend(final_cells)

# Save the final notebook
with open('/workspace/diabetic_retinopathy_detection.ipynb', 'w') as f:
    json.dump(notebook, f, indent=1)

print("Final sections added successfully!")
print("Complete notebook saved to: /workspace/diabetic_retinopathy_detection.ipynb")