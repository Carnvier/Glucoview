{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Diabetic Retinopathy Detection Pipeline\n",
    "## IET Codefest 2025 - Complete ML Pipeline\n",
    "\n",
    "This notebook implements a comprehensive diabetic retinopathy detection system using PyTorch and ResNet50.\n",
    "\n",
    "**Pipeline Overview:**\n",
    "1. Dataset Understanding & Label Cleaning\n",
    "2. Exploratory Data Analysis (EDA)\n",
    "3. Preprocessing & Augmentation\n",
    "4. Model Training (Two-phase ResNet50)\n",
    "5. Explainability (Grad-CAM)\n",
    "6. Model Export (ONNX for Next.js)\n",
    "7. Comprehensive Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \ud83d\udce6 Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Install required packages\n",
    "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "!pip install timm albumentations opencv-python-headless\n",
    "!pip install pytorch-grad-cam onnx onnxruntime scikit-learn\n",
    "!pip install matplotlib seaborn plotly pandas numpy\n",
    "!pip install efficientnet-pytorch"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import os\n",
    "import json\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "import re\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import models\n",
    "import timm\n",
    "\n",
    "# Image processing\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "# ML utilities\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (\n",
    "    classification_report, confusion_matrix, roc_curve, auc,\n",
    "    precision_recall_curve, f1_score, accuracy_score\n",
    ")\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "# Explainability\n",
    "from pytorch_grad_cam import GradCAM\n",
    "from pytorch_grad_cam.utils.model_targets import ClassifierOutputTarget\n",
    "from pytorch_grad_cam.utils.image import show_cam_on_image\n",
    "\n",
    "# ONNX export\n",
    "import onnx\n",
    "import onnxruntime as ort\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \u2699\ufe0f Configuration"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Configuration\n",
    "CONFIG = {\n",
    "    'DATA_PATH': '/kaggle/input',  # Update this path to your dataset location\n",
    "    'LABELS_FILE': 'labels.csv',  # Update this to your labels file name\n",
    "    'IMAGE_SIZE': 224,\n",
    "    'BATCH_SIZE': 32,\n",
    "    'NUM_EPOCHS': 50,\n",
    "    'LEARNING_RATE': 1e-4,\n",
    "    'WEIGHT_DECAY': 1e-5,\n",
    "    'NUM_CLASSES': 5,\n",
    "    'MODEL_NAME': 'resnet50',\n",
    "    'PATIENCE': 10,\n",
    "    'MIN_DELTA': 0.001,\n",
    "    'SAVE_PATH': './models/',\n",
    "    'RANDOM_SEED': 42\n",
    "}\n",
    "\n",
    "# Create directories\n",
    "os.makedirs(CONFIG['SAVE_PATH'], exist_ok=True)\n",
    "os.makedirs('./outputs', exist_ok=True)\n",
    "\n",
    "print(\"Configuration:\")\n",
    "for key, value in CONFIG.items():\n",
    "    print(f\"{key}: {value}\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1\ufe0f\u20e3 Dataset Understanding & Label Cleaning\n",
    "\n",
    "First, we'll load the labels.csv file and clean the inconsistent labels into 5 standardized classes:\n",
    "- 0 = No_DR (No Diabetic Retinopathy)\n",
    "- 1 = Mild\n",
    "- 2 = Moderate\n",
    "- 3 = Severe\n",
    "- 4 = Proliferative_DR"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def clean_labels(df):\n",
    "    \"\"\"\n",
    "    Clean and normalize inconsistent labels into 5 standardized classes\n",
    "    \"\"\"\n",
    "    # Create a copy to avoid modifying original\n",
    "    df_clean = df.copy()\n",
    "    \n",
    "    # Convert to string and strip whitespace\n",
    "    df_clean['label'] = df_clean['label'].astype(str).str.strip()\n",
    "    \n",
    "    # Define mapping for various label formats\n",
    "    label_mapping = {\n",
    "        # Numeric labels\n",
    "        '0': 0, '00': 0, '0.0': 0,\n",
    "        '1': 1, '01': 1, '1.0': 1,\n",
    "        '2': 2, '02': 2, '2.0': 2,\n",
    "        '3': 3, '03': 3, '3.0': 3,\n",
    "        '4': 4, '04': 4, '4.0': 4,\n",
    "        \n",
    "        # Text labels (case insensitive)\n",
    "        'NO_DR': 0, 'No_DR': 0, 'no_dr': 0, 'No DR': 0, 'no dr': 0,\n",
    "        'MILD': 1, 'Mild': 1, 'mild': 1,\n",
    "        'MODERATE': 2, 'Moderate': 2, 'moderate': 2,\n",
    "        'SEVERE': 3, 'Severe': 3, 'severe': 3,\n",
    "        'PROLIFERATIVE_DR': 4, 'Proliferative_DR': 4, 'proliferative_dr': 4,\n",
    "        'PROLIFERATIVE DR': 4, 'Proliferative DR': 4, 'proliferative dr': 4\n",
    "    }\n",
    "    \n",
    "    # Apply mapping\n",
    "    df_clean['label_clean'] = df_clean['label'].map(label_mapping)\n",
    "    \n",
    "    # Identify invalid labels\n",
    "    invalid_mask = df_clean['label_clean'].isna()\n",
    "    invalid_labels = df_clean[invalid_mask]['label'].unique()\n",
    "    \n",
    "    print(f\"Found {invalid_mask.sum()} invalid labels: {invalid_labels}\")\n",
    "    \n",
    "    # Remove invalid labels\n",
    "    df_clean = df_clean[~invalid_mask].copy()\n",
    "    \n",
    "    # Convert to int\n",
    "    df_clean['label_clean'] = df_clean['label_clean'].astype(int)\n",
    "    \n",
    "    return df_clean, invalid_labels\n",
    "\n",
    "# Load and clean labels\n",
    "print(\"Loading labels.csv...\")\n",
    "try:\n",
    "    # Try to find labels file in various locations\n",
    "    possible_paths = [\n",
    "        os.path.join(CONFIG['DATA_PATH'], CONFIG['LABELS_FILE']),\n",
    "        CONFIG['LABELS_FILE'],\n",
    "        './labels.csv',\n",
    "        '../input/labels.csv'\n",
    "    ]\n",
    "    \n",
    "    labels_df = None\n",
    "    for path in possible_paths:\n",
    "        if os.path.exists(path):\n",
    "            labels_df = pd.read_csv(path)\n",
    "            print(f\"Found labels file at: {path}\")\n",
    "            break\n",
    "    \n",
    "    if labels_df is None:\n",
    "        raise FileNotFoundError(\"labels.csv not found. Please update CONFIG['DATA_PATH'] and CONFIG['LABELS_FILE']\")\n",
    "    \n",
    "    print(f\"Original dataset shape: {labels_df.shape}\")\n",
    "    print(f\"Columns: {list(labels_df.columns)}\")\n",
    "    \n",
    "    # Display first few rows\n",
    "    print(\"\\nFirst 10 rows:\")\n",
    "    print(labels_df.head(10))\n",
    "    \n",
    "    # Show unique labels before cleaning\n",
    "    print(f\"\\nUnique labels before cleaning: {sorted(labels_df['label'].unique())}\")\n",
    "    \n",
    "    # Clean labels\n",
    "    labels_clean, invalid_labels = clean_labels(labels_df)\n",
    "    \n",
    "    print(f\"\\nCleaned dataset shape: {labels_clean.shape}\")\n",
    "    print(f\"Removed {len(labels_df) - len(labels_clean)} invalid entries\")\n",
    "    \n",
    "except FileNotFoundError as e:\n",
    "    print(f\"Error: {e}\")\n",
    "    print(\"Creating sample dataset for demonstration...\")\n",
    "    \n",
    "    # Create sample dataset for demo\n",
    "    sample_data = {\n",
    "        'image_id': [f'img_{i:04d}.jpg' for i in range(1000)],\n",
    "        'label': np.random.choice(['0', '1', '2', '3', '4', 'No_DR', 'Mild', 'unknown'], 1000)\n",
    "    }\n",
    "    labels_df = pd.DataFrame(sample_data)\n",
    "    labels_clean, invalid_labels = clean_labels(labels_df)\n",
    "    print(\"Sample dataset created for demonstration.\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Display class distribution\n",
    "class_names = ['No_DR', 'Mild', 'Moderate', 'Severe', 'Proliferative_DR']\n",
    "class_counts = labels_clean['label_clean'].value_counts().sort_index()\n",
    "\n",
    "print(\"\\nClass Distribution:\")\n",
    "print(\"=\" * 40)\n",
    "for i, (class_id, count) in enumerate(class_counts.items()):\n",
    "    percentage = count / len(labels_clean) * 100\n",
    "    print(f\"{class_id}: {class_names[class_id]:<15} {count:>6} ({percentage:>5.1f}%)\")\n",
    "\n",
    "print(f\"\\nTotal valid samples: {len(labels_clean)}\")\n",
    "\n",
    "# Visualize class distribution\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "class_counts.plot(kind='bar', color='skyblue', edgecolor='black')\n",
    "plt.title('Class Distribution (Count)')\n",
    "plt.xlabel('Class')\n",
    "plt.ylabel('Count')\n",
    "plt.xticks(range(len(class_names)), class_names, rotation=45)\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "percentages = class_counts / len(labels_clean) * 100\n",
    "plt.pie(percentages, labels=class_names, autopct='%1.1f%%', startangle=90)\n",
    "plt.title('Class Distribution (Percentage)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Check for class imbalance\n",
    "imbalance_ratio = class_counts.max() / class_counts.min()\n",
    "print(f\"\\nClass imbalance ratio: {imbalance_ratio:.2f}\")\n",
    "if imbalance_ratio > 5:\n",
    "    print(\"\u26a0\ufe0f  Significant class imbalance detected. Will use weighted sampling/loss.\")\n",
    "else:\n",
    "    print(\"\u2705 Class distribution is relatively balanced.\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \ud83d\udd0d Image File Verification\n",
    "\n",
    "Let's check if all labeled images exist and identify any missing files."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def find_images(labels_df, data_path):\n",
    "    \"\"\"\n",
    "    Find image files and check for missing images\n",
    "    \"\"\"\n",
    "    # Common image extensions\n",
    "    extensions = ['.jpg', '.jpeg', '.png', '.tiff', '.tif']\n",
    "    \n",
    "    # Look for images in various subdirectories\n",
    "    search_paths = [\n",
    "        data_path,\n",
    "        os.path.join(data_path, 'images'),\n",
    "        os.path.join(data_path, 'train'),\n",
    "        os.path.join(data_path, 'test'),\n",
    "        './images',\n",
    "        '../input/images'\n",
    "    ]\n",
    "    \n",
    "    image_files = {}\n",
    "    image_dir = None\n",
    "    \n",
    "    for search_path in search_paths:\n",
    "        if os.path.exists(search_path):\n",
    "            for ext in extensions:\n",
    "                pattern = f\"*{ext}\"\n",
    "                files = list(Path(search_path).glob(pattern))\n",
    "                if files:\n",
    "                    image_dir = search_path\n",
    "                    for file in files:\n",
    "                        image_files[file.name] = str(file)\n",
    "                    print(f\"Found {len(files)} {ext} files in {search_path}\")\n",
    "    \n",
    "    if not image_files:\n",
    "        print(\"No image files found. Please check your data path.\")\n",
    "        return None, None, None\n",
    "    \n",
    "    # Check for missing images\n",
    "    missing_images = []\n",
    "    existing_images = []\n",
    "    \n",
    "    for img_id in labels_df['image_id']:\n",
    "        if img_id in image_files:\n",
    "            existing_images.append(img_id)\n",
    "        else:\n",
    "            # Try with different extensions\n",
    "            base_name = os.path.splitext(img_id)[0]\n",
    "            found = False\n",
    "            for ext in extensions:\n",
    "                if f\"{base_name}{ext}\" in image_files:\n",
    "                    existing_images.append(img_id)\n",
    "                    found = True\n",
    "                    break\n",
    "            if not found:\n",
    "                missing_images.append(img_id)\n",
    "    \n",
    "    return image_files, existing_images, missing_images, image_dir\n",
    "\n",
    "# Find images\n",
    "print(\"Searching for image files...\")\n",
    "image_files, existing_images, missing_images, image_dir = find_images(labels_clean, CONFIG['DATA_PATH'])\n",
    "\n",
    "if image_files:\n",
    "    print(f\"\\nImage File Summary:\")\n",
    "    print(f\"Total image files found: {len(image_files)}\")\n",
    "    print(f\"Images with labels: {len(existing_images)}\")\n",
    "    print(f\"Missing images: {len(missing_images)}\")\n",
    "    \n",
    "    if missing_images:\n",
    "        print(f\"\\nFirst 10 missing images: {missing_images[:10]}\")\n",
    "        \n",
    "        # Filter out missing images\n",
    "        labels_final = labels_clean[labels_clean['image_id'].isin(existing_images)].copy()\n",
    "        print(f\"Final dataset size after removing missing images: {len(labels_final)}\")\n",
    "    else:\n",
    "        labels_final = labels_clean.copy()\n",
    "        print(\"\u2705 All labeled images found!\")\n",
    "    \n",
    "    # Update config with image directory\n",
    "    CONFIG['IMAGE_DIR'] = image_dir\n",
    "    print(f\"Image directory: {image_dir}\")\n",
    "    \n",
    "else:\n",
    "    print(\"\u26a0\ufe0f  No images found. Using labels only for demonstration.\")\n",
    "    labels_final = labels_clean.copy()\n",
    "    CONFIG['IMAGE_DIR'] = None"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \ud83d\udcf8 Sample Images Visualization\n",
    "\n",
    "Let's display random samples from each class to understand the data better."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def load_and_preprocess_image(image_path, size=224):\n",
    "    \"\"\"\n",
    "    Load and preprocess image for display\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Load image\n",
    "        img = cv2.imread(image_path)\n",
    "        if img is None:\n",
    "            return None\n",
    "        \n",
    "        # Convert BGR to RGB\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # Resize\n",
    "        img = cv2.resize(img, (size, size))\n",
    "        \n",
    "        return img\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading image {image_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "def crop_black_borders(image, threshold=10):\n",
    "    \"\"\"\n",
    "    Crop black borders from retinal images\n",
    "    \"\"\"\n",
    "    # Convert to grayscale for border detection\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n",
    "    \n",
    "    # Find non-black pixels\n",
    "    coords = cv2.findNonZero((gray > threshold).astype(np.uint8))\n",
    "    \n",
    "    if coords is not None:\n",
    "        # Get bounding box\n",
    "        x, y, w, h = cv2.boundingRect(coords)\n",
    "        \n",
    "        # Add small padding\n",
    "        pad = 5\n",
    "        x = max(0, x - pad)\n",
    "        y = max(0, y - pad)\n",
    "        w = min(image.shape[1] - x, w + 2*pad)\n",
    "        h = min(image.shape[0] - y, h + 2*pad)\n",
    "        \n",
    "        # Crop image\n",
    "        cropped = image[y:y+h, x:x+w]\n",
    "        return cropped\n",
    "    \n",
    "    return image\n",
    "\n",
    "# Display sample images if available\n",
    "if CONFIG['IMAGE_DIR'] and len(labels_final) > 0:\n",
    "    print(\"Displaying sample images from each class...\")\n",
    "    \n",
    "    fig, axes = plt.subplots(5, 3, figsize=(12, 20))\n",
    "    fig.suptitle('Sample Images by Class (Original, Cropped, Resized)', fontsize=16)\n",
    "    \n",
    "    for class_id in range(5):\n",
    "        # Get samples from this class\n",
    "        class_samples = labels_final[labels_final['label_clean'] == class_id].sample(\n",
    "            min(1, len(labels_final[labels_final['label_clean'] == class_id])), \n",
    "            random_state=42\n",
    "        )\n",
    "        \n",
    "        if len(class_samples) > 0:\n",
    "            img_id = class_samples.iloc[0]['image_id']\n",
    "            \n",
    "            # Find image path\n",
    "            img_path = None\n",
    "            if img_id in image_files:\n",
    "                img_path = image_files[img_id]\n",
    "            else:\n",
    "                # Try different extensions\n",
    "                base_name = os.path.splitext(img_id)[0]\n",
    "                for ext in ['.jpg', '.jpeg', '.png']:\n",
    "                    if f\"{base_name}{ext}\" in image_files:\n",
    "                        img_path = image_files[f\"{base_name}{ext}\"]\n",
    "                        break\n",
    "            \n",
    "            if img_path and os.path.exists(img_path):\n",
    "                # Load original image\n",
    "                original_img = load_and_preprocess_image(img_path, size=300)\n",
    "                \n",
    "                if original_img is not None:\n",
    "                    # Crop borders\n",
    "                    cropped_img = crop_black_borders(original_img)\n",
    "                    \n",
    "                    # Resize to final size\n",
    "                    final_img = cv2.resize(cropped_img, (224, 224))\n",
    "                    \n",
    "                    # Display images\n",
    "                    axes[class_id, 0].imshow(original_img)\n",
    "                    axes[class_id, 0].set_title(f'{class_names[class_id]}\\nOriginal')\n",
    "                    axes[class_id, 0].axis('off')\n",
    "                    \n",
    "                    axes[class_id, 1].imshow(cropped_img)\n",
    "                    axes[class_id, 1].set_title('Cropped')\n",
    "                    axes[class_id, 1].axis('off')\n",
    "                    \n",
    "                    axes[class_id, 2].imshow(final_img)\n",
    "                    axes[class_id, 2].set_title('Resized (224x224)')\n",
    "                    axes[class_id, 2].axis('off')\n",
    "                    \n",
    "                    continue\n",
    "        \n",
    "        # If no image found, show placeholder\n",
    "        for j in range(3):\n",
    "            axes[class_id, j].text(0.5, 0.5, f'{class_names[class_id]}\\nNo image', \n",
    "                                 ha='center', va='center', transform=axes[class_id, j].transAxes)\n",
    "            axes[class_id, j].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "else:\n",
    "    print(\"No images available for visualization.\")\n",
    "    print(\"The pipeline will continue with data loading and model training structure.\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2\ufe0f\u20e3 Preprocessing & Augmentation\n",
    "\n",
    "We'll create a custom dataset class with:\n",
    "- Black border cropping\n",
    "- Image resizing to 224\u00d7224\n",
    "- ImageNet normalization\n",
    "- Data augmentation for training"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "class DiabeticRetinopathyDataset(Dataset):\n",
    "    def __init__(self, dataframe, image_dir, transform=None, is_training=True):\n",
    "        self.dataframe = dataframe.reset_index(drop=True)\n",
    "        self.image_dir = image_dir\n",
    "        self.transform = transform\n",
    "        self.is_training = is_training\n",
    "        \n",
    "        # ImageNet statistics\n",
    "        self.mean = [0.485, 0.456, 0.406]\n",
    "        self.std = [0.229, 0.224, 0.225]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.dataframe.iloc[idx]\n",
    "        img_id = row['image_id']\n",
    "        label = row['label_clean']\n",
    "        \n",
    "        # Load image\n",
    "        img_path = self._find_image_path(img_id)\n",
    "        \n",
    "        if img_path is None:\n",
    "            # Return dummy image if not found\n",
    "            image = np.zeros((224, 224, 3), dtype=np.uint8)\n",
    "        else:\n",
    "            image = self._load_image(img_path)\n",
    "        \n",
    "        # Apply transformations\n",
    "        if self.transform:\n",
    "            transformed = self.transform(image=image)\n",
    "            image = transformed['image']\n",
    "        \n",
    "        return image, torch.tensor(label, dtype=torch.long)\n",
    "    \n",
    "    def _find_image_path(self, img_id):\n",
    "        \"\"\"Find the full path to an image\"\"\"\n",
    "        if self.image_dir is None:\n",
    "            return None\n",
    "        \n",
    "        # Try exact match first\n",
    "        exact_path = os.path.join(self.image_dir, img_id)\n",
    "        if os.path.exists(exact_path):\n",
    "            return exact_path\n",
    "        \n",
    "        # Try different extensions\n",
    "        base_name = os.path.splitext(img_id)[0]\n",
    "        for ext in ['.jpg', '.jpeg', '.png', '.tiff', '.tif']:\n",
    "            path = os.path.join(self.image_dir, f\"{base_name}{ext}\")\n",
    "            if os.path.exists(path):\n",
    "                return path\n",
    "        \n",
    "        return None\n",
    "    \n",
    "    def _load_image(self, img_path):\n",
    "        \"\"\"Load and preprocess image\"\"\"\n",
    "        try:\n",
    "            # Load image\n",
    "            img = cv2.imread(img_path)\n",
    "            if img is None:\n",
    "                raise ValueError(f\"Could not load image: {img_path}\")\n",
    "            \n",
    "            # Convert BGR to RGB\n",
    "            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "            \n",
    "            # Crop black borders\n",
    "            img = self._crop_black_borders(img)\n",
    "            \n",
    "            # Resize\n",
    "            img = cv2.resize(img, (CONFIG['IMAGE_SIZE'], CONFIG['IMAGE_SIZE']))\n",
    "            \n",
    "            return img\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error loading image {img_path}: {e}\")\n",
    "            # Return black image as fallback\n",
    "            return np.zeros((CONFIG['IMAGE_SIZE'], CONFIG['IMAGE_SIZE'], 3), dtype=np.uint8)\n",
    "    \n",
    "    def _crop_black_borders(self, image, threshold=10):\n",
    "        \"\"\"Crop black borders from retinal images\"\"\"\n",
    "        # Convert to grayscale for border detection\n",
    "        gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n",
    "        \n",
    "        # Find non-black pixels\n",
    "        coords = cv2.findNonZero((gray > threshold).astype(np.uint8))\n",
    "        \n",
    "        if coords is not None:\n",
    "            # Get bounding box\n",
    "            x, y, w, h = cv2.boundingRect(coords)\n",
    "            \n",
    "            # Add small padding\n",
    "            pad = 5\n",
    "            x = max(0, x - pad)\n",
    "            y = max(0, y - pad)\n",
    "            w = min(image.shape[1] - x, w + 2*pad)\n",
    "            h = min(image.shape[0] - y, h + 2*pad)\n",
    "            \n",
    "            # Crop image\n",
    "            cropped = image[y:y+h, x:x+w]\n",
    "            return cropped\n",
    "        \n",
    "        return image\n",
    "\n",
    "# Define transforms\n",
    "def get_transforms(is_training=True):\n",
    "    \"\"\"Get image transforms for training/validation\"\"\"\n",
    "    \n",
    "    if is_training:\n",
    "        transform = A.Compose([\n",
    "            A.HorizontalFlip(p=0.5),\n",
    "            A.VerticalFlip(p=0.5),\n",
    "            A.Rotate(limit=15, p=0.5),\n",
    "            A.RandomBrightnessContrast(\n",
    "                brightness_limit=0.2, \n",
    "                contrast_limit=0.2, \n",
    "                p=0.5\n",
    "            ),\n",
    "            A.HueSaturationValue(\n",
    "                hue_shift_limit=10,\n",
    "                sat_shift_limit=20,\n",
    "                val_shift_limit=10,\n",
    "                p=0.3\n",
    "            ),\n",
    "            A.GaussNoise(var_limit=(10.0, 50.0), p=0.3),\n",
    "            A.Normalize(\n",
    "                mean=[0.485, 0.456, 0.406],\n",
    "                std=[0.229, 0.224, 0.225],\n",
    "                max_pixel_value=255.0\n",
    "            ),\n",
    "            ToTensorV2()\n",
    "        ])\n",
    "    else:\n",
    "        transform = A.Compose([\n",
    "            A.Normalize(\n",
    "                mean=[0.485, 0.456, 0.406],\n",
    "                std=[0.229, 0.224, 0.225],\n",
    "                max_pixel_value=255.0\n",
    "            ),\n",
    "            ToTensorV2()\n",
    "        ])\n",
    "    \n",
    "    return transform\n",
    "\n",
    "print(\"Dataset class and transforms defined successfully!\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Split data into train, validation, and test sets\n",
    "train_df, temp_df = train_test_split(\n",
    "    labels_final, \n",
    "    test_size=0.3, \n",
    "    random_state=CONFIG['RANDOM_SEED'],\n",
    "    stratify=labels_final['label_clean']\n",
    ")\n",
    "\n",
    "val_df, test_df = train_test_split(\n",
    "    temp_df, \n",
    "    test_size=0.5, \n",
    "    random_state=CONFIG['RANDOM_SEED'],\n",
    "    stratify=temp_df['label_clean']\n",
    ")\n",
    "\n",
    "print(f\"Dataset splits:\")\n",
    "print(f\"Train: {len(train_df)} samples\")\n",
    "print(f\"Validation: {len(val_df)} samples\")\n",
    "print(f\"Test: {len(test_df)} samples\")\n",
    "\n",
    "# Display class distribution in each split\n",
    "splits_info = {\n",
    "    'Train': train_df['label_clean'].value_counts().sort_index(),\n",
    "    'Validation': val_df['label_clean'].value_counts().sort_index(),\n",
    "    'Test': test_df['label_clean'].value_counts().sort_index()\n",
    "}\n",
    "\n",
    "print(\"\\nClass distribution by split:\")\n",
    "for split_name, counts in splits_info.items():\n",
    "    print(f\"\\n{split_name}:\")\n",
    "    for class_id, count in counts.items():\n",
    "        print(f\"  {class_names[class_id]}: {count}\")\n",
    "\n",
    "# Calculate class weights for handling imbalance\n",
    "class_weights = compute_class_weight(\n",
    "    'balanced',\n",
    "    classes=np.unique(train_df['label_clean']),\n",
    "    y=train_df['label_clean']\n",
    ")\n",
    "class_weights_tensor = torch.FloatTensor(class_weights).to(device)\n",
    "\n",
    "print(f\"\\nClass weights for balanced loss:\")\n",
    "for i, weight in enumerate(class_weights):\n",
    "    print(f\"{class_names[i]}: {weight:.3f}\")\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = DiabeticRetinopathyDataset(\n",
    "    train_df, \n",
    "    CONFIG['IMAGE_DIR'], \n",
    "    transform=get_transforms(is_training=True),\n",
    "    is_training=True\n",
    ")\n",
    "\n",
    "val_dataset = DiabeticRetinopathyDataset(\n",
    "    val_df, \n",
    "    CONFIG['IMAGE_DIR'], \n",
    "    transform=get_transforms(is_training=False),\n",
    "    is_training=False\n",
    ")\n",
    "\n",
    "test_dataset = DiabeticRetinopathyDataset(\n",
    "    test_df, \n",
    "    CONFIG['IMAGE_DIR'], \n",
    "    transform=get_transforms(is_training=False),\n",
    "    is_training=False\n",
    ")\n",
    "\n",
    "print(f\"\\nDatasets created successfully!\")\n",
    "print(f\"Train dataset: {len(train_dataset)} samples\")\n",
    "print(f\"Validation dataset: {len(val_dataset)} samples\")\n",
    "print(f\"Test dataset: {len(test_dataset)} samples\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Create weighted sampler for handling class imbalance\n",
    "def create_weighted_sampler(dataset, labels):\n",
    "    \"\"\"Create weighted sampler for imbalanced dataset\"\"\"\n",
    "    class_counts = np.bincount(labels)\n",
    "    class_weights = 1.0 / class_counts\n",
    "    sample_weights = [class_weights[label] for label in labels]\n",
    "    \n",
    "    return WeightedRandomSampler(\n",
    "        weights=sample_weights,\n",
    "        num_samples=len(sample_weights),\n",
    "        replacement=True\n",
    "    )\n",
    "\n",
    "# Create weighted sampler for training\n",
    "train_sampler = create_weighted_sampler(train_dataset, train_df['label_clean'].values)\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=CONFIG['BATCH_SIZE'],\n",
    "    sampler=train_sampler,\n",
    "    num_workers=4,\n",
    "    pin_memory=True,\n",
    "    drop_last=True\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=CONFIG['BATCH_SIZE'],\n",
    "    shuffle=False,\n",
    "    num_workers=4,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=CONFIG['BATCH_SIZE'],\n",
    "    shuffle=False,\n",
    "    num_workers=4,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "print(f\"Data loaders created:\")\n",
    "print(f\"Train batches: {len(train_loader)}\")\n",
    "print(f\"Validation batches: {len(val_loader)}\")\n",
    "print(f\"Test batches: {len(test_loader)}\")\n",
    "\n",
    "# Test data loading\n",
    "print(\"\\nTesting data loading...\")\n",
    "try:\n",
    "    sample_batch = next(iter(train_loader))\n",
    "    images, labels = sample_batch\n",
    "    print(f\"Batch shape: {images.shape}\")\n",
    "    print(f\"Labels shape: {labels.shape}\")\n",
    "    print(f\"Image range: [{images.min():.3f}, {images.max():.3f}]\")\n",
    "    print(f\"Sample labels: {labels[:10].tolist()}\")\n",
    "    print(\"\u2705 Data loading successful!\")\n",
    "except Exception as e:\n",
    "    print(f\"\u274c Data loading error: {e}\")\n",
    "    print(\"Note: This is expected if no images are available.\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3\ufe0f\u20e3 Model Architecture (ResNet50)\n",
    "\n",
    "We'll use a two-phase training approach:\n",
    "1. **Phase 1**: Freeze backbone, train classifier head only\n",
    "2. **Phase 2**: Unfreeze backbone, fine-tune with smaller learning rate"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "class DiabeticRetinopathyModel(nn.Module):\n",
    "    def __init__(self, num_classes=5, model_name='resnet50', pretrained=True):\n",
    "        super(DiabeticRetinopathyModel, self).__init__()\n",
    "        \n",
    "        # Load pretrained model\n",
    "        if model_name == 'resnet50':\n",
    "            self.backbone = models.resnet50(pretrained=pretrained)\n",
    "            num_features = self.backbone.fc.in_features\n",
    "            self.backbone.fc = nn.Identity()  # Remove final layer\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported model: {model_name}\")\n",
    "        \n",
    "        # Custom classifier head\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(num_features, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(512, num_classes)\n",
    "        )\n",
    "        \n",
    "        self.num_classes = num_classes\n",
    "        self.model_name = model_name\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Extract features\n",
    "        features = self.backbone(x)\n",
    "        \n",
    "        # Classify\n",
    "        output = self.classifier(features)\n",
    "        \n",
    "        return output\n",
    "    \n",
    "    def freeze_backbone(self):\n",
    "        \"\"\"Freeze backbone parameters\"\"\"\n",
    "        for param in self.backbone.parameters():\n",
    "            param.requires_grad = False\n",
    "    \n",
    "    def unfreeze_backbone(self):\n",
    "        \"\"\"Unfreeze backbone parameters\"\"\"\n",
    "        for param in self.backbone.parameters():\n",
    "            param.requires_grad = True\n",
    "    \n",
    "    def get_trainable_params(self):\n",
    "        \"\"\"Get number of trainable parameters\"\"\"\n",
    "        return sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
    "\n",
    "# Create model\n",
    "model = DiabeticRetinopathyModel(\n",
    "    num_classes=CONFIG['NUM_CLASSES'],\n",
    "    model_name=CONFIG['MODEL_NAME'],\n",
    "    pretrained=True\n",
    ").to(device)\n",
    "\n",
    "print(f\"Model created: {CONFIG['MODEL_NAME']}\")\n",
    "print(f\"Total parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "print(f\"Trainable parameters: {model.get_trainable_params():,}\")\n",
    "\n",
    "# Test model forward pass\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    dummy_input = torch.randn(2, 3, CONFIG['IMAGE_SIZE'], CONFIG['IMAGE_SIZE']).to(device)\n",
    "    dummy_output = model(dummy_input)\n",
    "    print(f\"\\nModel output shape: {dummy_output.shape}\")\n",
    "    print(f\"Output range: [{dummy_output.min():.3f}, {dummy_output.max():.3f}]\")\n",
    "    print(\"\u2705 Model forward pass successful!\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Training utilities\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=10, min_delta=0.001, restore_best_weights=True):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.restore_best_weights = restore_best_weights\n",
    "        self.best_loss = None\n",
    "        self.counter = 0\n",
    "        self.best_weights = None\n",
    "    \n",
    "    def __call__(self, val_loss, model):\n",
    "        if self.best_loss is None:\n",
    "            self.best_loss = val_loss\n",
    "            self.save_checkpoint(model)\n",
    "        elif val_loss < self.best_loss - self.min_delta:\n",
    "            self.best_loss = val_loss\n",
    "            self.counter = 0\n",
    "            self.save_checkpoint(model)\n",
    "        else:\n",
    "            self.counter += 1\n",
    "        \n",
    "        if self.counter >= self.patience:\n",
    "            if self.restore_best_weights:\n",
    "                model.load_state_dict(self.best_weights)\n",
    "            return True\n",
    "        return False\n",
    "    \n",
    "    def save_checkpoint(self, model):\n",
    "        self.best_weights = model.state_dict().copy()\n",
    "\n",
    "def calculate_metrics(y_true, y_pred, y_prob=None):\n",
    "    \"\"\"Calculate comprehensive metrics\"\"\"\n",
    "    from sklearn.metrics import (\n",
    "        accuracy_score, precision_recall_fscore_support,\n",
    "        roc_auc_score, confusion_matrix\n",
    "    )\n",
    "    \n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(y_true, y_pred, average='weighted')\n",
    "    \n",
    "    metrics = {\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1\n",
    "    }\n",
    "    \n",
    "    # Calculate AUC if probabilities provided\n",
    "    if y_prob is not None:\n",
    "        try:\n",
    "            auc_score = roc_auc_score(y_true, y_prob, multi_class='ovr', average='weighted')\n",
    "            metrics['auc'] = auc_score\n",
    "        except:\n",
    "            metrics['auc'] = 0.0\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "def train_epoch(model, loader, criterion, optimizer, device):\n",
    "    \"\"\"Train for one epoch\"\"\"\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    all_probs = []\n",
    "    \n",
    "    for images, labels in loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        # Get predictions\n",
    "        probs = torch.softmax(outputs, dim=1)\n",
    "        preds = torch.argmax(outputs, dim=1)\n",
    "        \n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "        all_probs.extend(probs.cpu().numpy())\n",
    "    \n",
    "    epoch_loss = running_loss / len(loader)\n",
    "    metrics = calculate_metrics(all_labels, all_preds, all_probs)\n",
    "    \n",
    "    return epoch_loss, metrics\n",
    "\n",
    "def validate_epoch(model, loader, criterion, device):\n",
    "    \"\"\"Validate for one epoch\"\"\"\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    all_probs = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            \n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            \n",
    "            # Get predictions\n",
    "            probs = torch.softmax(outputs, dim=1)\n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "            \n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_probs.extend(probs.cpu().numpy())\n",
    "    \n",
    "    epoch_loss = running_loss / len(loader)\n",
    "    metrics = calculate_metrics(all_labels, all_preds, all_probs)\n",
    "    \n",
    "    return epoch_loss, metrics, all_labels, all_preds, all_probs\n",
    "\n",
    "print(\"Training utilities defined successfully!\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \ud83c\udfaf Phase 1: Train Classifier Head (Frozen Backbone)\n",
    "\n",
    "First, we'll freeze the ResNet50 backbone and train only the classifier head."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Phase 1: Freeze backbone and train classifier\n",
    "print(\"=\" * 60)\n",
    "print(\"PHASE 1: Training Classifier Head (Frozen Backbone)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Freeze backbone\n",
    "model.freeze_backbone()\n",
    "print(f\"Trainable parameters: {model.get_trainable_params():,}\")\n",
    "\n",
    "# Setup training\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights_tensor)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=CONFIG['LEARNING_RATE'], weight_decay=CONFIG['WEIGHT_DECAY'])\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5, verbose=True)\n",
    "early_stopping = EarlyStopping(patience=CONFIG['PATIENCE'], min_delta=CONFIG['MIN_DELTA'])\n",
    "\n",
    "# Training history\n",
    "history = {\n",
    "    'train_loss': [], 'val_loss': [],\n",
    "    'train_acc': [], 'val_acc': [],\n",
    "    'train_f1': [], 'val_f1': [],\n",
    "    'val_auc': []\n",
    "}\n",
    "\n",
    "print(f\"\\nStarting Phase 1 training for up to {CONFIG['NUM_EPOCHS']} epochs...\")\n",
    "print(f\"Learning rate: {CONFIG['LEARNING_RATE']}\")\n",
    "print(f\"Batch size: {CONFIG['BATCH_SIZE']}\")\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "phase1_epochs = 0\n",
    "\n",
    "for epoch in range(CONFIG['NUM_EPOCHS']):\n",
    "    print(f\"\\nEpoch {epoch+1}/{CONFIG['NUM_EPOCHS']}\")\n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "    # Train\n",
    "    train_loss, train_metrics = train_epoch(model, train_loader, criterion, optimizer, device)\n",
    "    \n",
    "    # Validate\n",
    "    val_loss, val_metrics, _, _, _ = validate_epoch(model, val_loader, criterion, device)\n",
    "    \n",
    "    # Update learning rate\n",
    "    scheduler.step(val_loss)\n",
    "    \n",
    "    # Store history\n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['val_loss'].append(val_loss)\n",
    "    history['train_acc'].append(train_metrics['accuracy'])\n",
    "    history['val_acc'].append(val_metrics['accuracy'])\n",
    "    history['train_f1'].append(train_metrics['f1'])\n",
    "    history['val_f1'].append(val_metrics['f1'])\n",
    "    history['val_auc'].append(val_metrics.get('auc', 0.0))\n",
    "    \n",
    "    # Print metrics\n",
    "    print(f\"Train Loss: {train_loss:.4f} | Train Acc: {train_metrics['accuracy']:.4f} | Train F1: {train_metrics['f1']:.4f}\")\n",
    "    print(f\"Val Loss:   {val_loss:.4f} | Val Acc:   {val_metrics['accuracy']:.4f} | Val F1:   {val_metrics['f1']:.4f} | Val AUC: {val_metrics.get('auc', 0.0):.4f}\")\n",
    "    \n",
    "    # Early stopping\n",
    "    if early_stopping(val_loss, model):\n",
    "        print(f\"\\nEarly stopping triggered after {epoch+1} epochs\")\n",
    "        break\n",
    "    \n",
    "    # Save best model\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'val_loss': val_loss,\n",
    "            'val_metrics': val_metrics\n",
    "        }, os.path.join(CONFIG['SAVE_PATH'], 'best_phase1_model.pth'))\n",
    "        print(f\"\u2705 Best model saved (Val Loss: {val_loss:.4f})\")\n",
    "    \n",
    "    phase1_epochs = epoch + 1\n",
    "\n",
    "print(f\"\\nPhase 1 completed after {phase1_epochs} epochs\")\n",
    "print(f\"Best validation loss: {best_val_loss:.4f}\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \ud83d\udd25 Phase 2: Fine-tune Entire Model (Unfrozen Backbone)\n",
    "\n",
    "Now we'll unfreeze the backbone and fine-tune the entire model with a smaller learning rate."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Phase 2: Unfreeze and fine-tune entire model\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"PHASE 2: Fine-tuning Entire Model (Unfrozen Backbone)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Unfreeze backbone\n",
    "model.unfreeze_backbone()\n",
    "print(f\"Trainable parameters: {model.get_trainable_params():,}\")\n",
    "\n",
    "# Setup training with smaller learning rate\n",
    "fine_tune_lr = CONFIG['LEARNING_RATE'] / 10  # 10x smaller LR for fine-tuning\n",
    "optimizer = optim.AdamW(model.parameters(), lr=fine_tune_lr, weight_decay=CONFIG['WEIGHT_DECAY'])\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3, verbose=True)\n",
    "early_stopping = EarlyStopping(patience=CONFIG['PATIENCE']//2, min_delta=CONFIG['MIN_DELTA']/2)  # More sensitive\n",
    "\n",
    "print(f\"\\nStarting Phase 2 training for up to {CONFIG['NUM_EPOCHS']//2} epochs...\")\n",
    "print(f\"Fine-tuning learning rate: {fine_tune_lr}\")\n",
    "\n",
    "phase2_epochs = 0\n",
    "best_val_loss_phase2 = float('inf')\n",
    "\n",
    "for epoch in range(CONFIG['NUM_EPOCHS']//2):  # Fewer epochs for fine-tuning\n",
    "    print(f\"\\nPhase 2 - Epoch {epoch+1}/{CONFIG['NUM_EPOCHS']//2}\")\n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "    # Train\n",
    "    train_loss, train_metrics = train_epoch(model, train_loader, criterion, optimizer, device)\n",
    "    \n",
    "    # Validate\n",
    "    val_loss, val_metrics, _, _, _ = validate_epoch(model, val_loader, criterion, device)\n",
    "    \n",
    "    # Update learning rate\n",
    "    scheduler.step(val_loss)\n",
    "    \n",
    "    # Store history\n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['val_loss'].append(val_loss)\n",
    "    history['train_acc'].append(train_metrics['accuracy'])\n",
    "    history['val_acc'].append(val_metrics['accuracy'])\n",
    "    history['train_f1'].append(train_metrics['f1'])\n",
    "    history['val_f1'].append(val_metrics['f1'])\n",
    "    history['val_auc'].append(val_metrics.get('auc', 0.0))\n",
    "    \n",
    "    # Print metrics\n",
    "    print(f\"Train Loss: {train_loss:.4f} | Train Acc: {train_metrics['accuracy']:.4f} | Train F1: {train_metrics['f1']:.4f}\")\n",
    "    print(f\"Val Loss:   {val_loss:.4f} | Val Acc:   {val_metrics['accuracy']:.4f} | Val F1:   {val_metrics['f1']:.4f} | Val AUC: {val_metrics.get('auc', 0.0):.4f}\")\n",
    "    \n",
    "    # Early stopping\n",
    "    if early_stopping(val_loss, model):\n",
    "        print(f\"\\nEarly stopping triggered after {epoch+1} epochs\")\n",
    "        break\n",
    "    \n",
    "    # Save best model\n",
    "    if val_loss < best_val_loss_phase2:\n",
    "        best_val_loss_phase2 = val_loss\n",
    "        torch.save({\n",
    "            'epoch': phase1_epochs + epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'val_loss': val_loss,\n",
    "            'val_metrics': val_metrics,\n",
    "            'history': history\n",
    "        }, os.path.join(CONFIG['SAVE_PATH'], 'best_final_model.pth'))\n",
    "        print(f\"\u2705 Best final model saved (Val Loss: {val_loss:.4f})\")\n",
    "    \n",
    "    phase2_epochs = epoch + 1\n",
    "\n",
    "print(f\"\\nPhase 2 completed after {phase2_epochs} epochs\")\n",
    "print(f\"Best validation loss: {best_val_loss_phase2:.4f}\")\n",
    "print(f\"\\nTotal training epochs: {phase1_epochs + phase2_epochs}\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Plot training history\n",
    "def plot_training_history(history):\n",
    "    \"\"\"Plot training and validation metrics\"\"\"\n",
    "    epochs = range(1, len(history['train_loss']) + 1)\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    fig.suptitle('Training History', fontsize=16)\n",
    "    \n",
    "    # Loss\n",
    "    axes[0, 0].plot(epochs, history['train_loss'], 'b-', label='Train Loss', linewidth=2)\n",
    "    axes[0, 0].plot(epochs, history['val_loss'], 'r-', label='Val Loss', linewidth=2)\n",
    "    axes[0, 0].set_title('Loss')\n",
    "    axes[0, 0].set_xlabel('Epoch')\n",
    "    axes[0, 0].set_ylabel('Loss')\n",
    "    axes[0, 0].legend()\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Accuracy\n",
    "    axes[0, 1].plot(epochs, history['train_acc'], 'b-', label='Train Acc', linewidth=2)\n",
    "    axes[0, 1].plot(epochs, history['val_acc'], 'r-', label='Val Acc', linewidth=2)\n",
    "    axes[0, 1].set_title('Accuracy')\n",
    "    axes[0, 1].set_xlabel('Epoch')\n",
    "    axes[0, 1].set_ylabel('Accuracy')\n",
    "    axes[0, 1].legend()\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # F1 Score\n",
    "    axes[1, 0].plot(epochs, history['train_f1'], 'b-', label='Train F1', linewidth=2)\n",
    "    axes[1, 0].plot(epochs, history['val_f1'], 'r-', label='Val F1', linewidth=2)\n",
    "    axes[1, 0].set_title('F1 Score')\n",
    "    axes[1, 0].set_xlabel('Epoch')\n",
    "    axes[1, 0].set_ylabel('F1 Score')\n",
    "    axes[1, 0].legend()\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # AUC\n",
    "    axes[1, 1].plot(epochs, history['val_auc'], 'g-', label='Val AUC', linewidth=2)\n",
    "    axes[1, 1].set_title('AUC Score')\n",
    "    axes[1, 1].set_xlabel('Epoch')\n",
    "    axes[1, 1].set_ylabel('AUC')\n",
    "    axes[1, 1].legend()\n",
    "    axes[1, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add phase separation line\n",
    "    if phase1_epochs > 0:\n",
    "        for ax in axes.flat:\n",
    "            ax.axvline(x=phase1_epochs, color='orange', linestyle='--', alpha=0.7, label='Phase 2 Start')\n",
    "            if ax == axes[0, 0]:  # Only add legend to first subplot\n",
    "                ax.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('./outputs/training_history.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "# Plot training history if we have data\n",
    "if len(history['train_loss']) > 0:\n",
    "    plot_training_history(history)\n",
    "    \n",
    "    # Print final metrics\n",
    "    print(\"\\nFinal Training Metrics:\")\n",
    "    print(f\"Best Validation Loss: {min(history['val_loss']):.4f}\")\n",
    "    print(f\"Best Validation Accuracy: {max(history['val_acc']):.4f}\")\n",
    "    print(f\"Best Validation F1: {max(history['val_f1']):.4f}\")\n",
    "    print(f\"Best Validation AUC: {max(history['val_auc']):.4f}\")\nelse:\n",
    "    print(\"No training history available. Skipping visualization.\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6\ufe0f\u20e3 Model Evaluation\n",
    "\n",
    "Let's evaluate our trained model on the test set and generate comprehensive metrics."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Load best model for evaluation\n",
    "try:\n",
    "    checkpoint = torch.load(os.path.join(CONFIG['SAVE_PATH'], 'best_final_model.pth'), map_location=device)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    print(\"\u2705 Loaded best final model for evaluation\")\n",
    "    print(f\"Model was saved at epoch {checkpoint['epoch']} with val loss {checkpoint['val_loss']:.4f}\")\nexcept:\n",
    "    print(\"\u26a0\ufe0f  No saved model found. Using current model state.\")\n",
    "\n",
    "# Evaluate on test set\n",
    "print(\"\\nEvaluating on test set...\")\n",
    "test_loss, test_metrics, test_labels, test_preds, test_probs = validate_epoch(model, test_loader, criterion, device)\n",
    "\n",
    "print(f\"\\nTest Results:\")\n",
    "print(f\"Test Loss: {test_loss:.4f}\")\n",
    "print(f\"Test Accuracy: {test_metrics['accuracy']:.4f}\")\n",
    "print(f\"Test Precision: {test_metrics['precision']:.4f}\")\n",
    "print(f\"Test Recall: {test_metrics['recall']:.4f}\")\n",
    "print(f\"Test F1: {test_metrics['f1']:.4f}\")\n",
    "print(f\"Test AUC: {test_metrics.get('auc', 0.0):.4f}\")\n",
    "\n",
    "# Detailed classification report\n",
    "from sklearn.metrics import classification_report\n",
    "print(\"\\nDetailed Classification Report:\")\n",
    "print(classification_report(test_labels, test_preds, target_names=class_names))"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Plot confusion matrix and ROC curves\n",
    "def plot_confusion_matrix(y_true, y_pred, class_names):\n",
    "    \"\"\"Plot confusion matrix\"\"\"\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    \n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=class_names, yticklabels=class_names)\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('Actual')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('./outputs/confusion_matrix.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    # Print normalized confusion matrix\n",
    "    cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "    \n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cm_normalized, annot=True, fmt='.3f', cmap='Blues',\n",
    "                xticklabels=class_names, yticklabels=class_names)\n",
    "    plt.title('Normalized Confusion Matrix')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('Actual')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('./outputs/confusion_matrix_normalized.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "def plot_roc_curves(y_true, y_prob, class_names):\n",
    "    \"\"\"Plot ROC curves for each class\"\"\"\n",
    "    from sklearn.preprocessing import label_binarize\n",
    "    from sklearn.metrics import roc_curve, auc\n",
    "    \n",
    "    # Binarize labels for multiclass ROC\n",
    "    y_true_bin = label_binarize(y_true, classes=range(len(class_names)))\n",
    "    \n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    colors = ['blue', 'red', 'green', 'orange', 'purple']\n",
    "    \n",
    "    for i, (class_name, color) in enumerate(zip(class_names, colors)):\n",
    "        if i < y_true_bin.shape[1] and i < len(y_prob[0]):\n",
    "            fpr, tpr, _ = roc_curve(y_true_bin[:, i], [prob[i] for prob in y_prob])\n",
    "            roc_auc = auc(fpr, tpr)\n",
    "            \n",
    "            plt.plot(fpr, tpr, color=color, lw=2,\n",
    "                    label=f'{class_name} (AUC = {roc_auc:.3f})')\n",
    "    \n",
    "    plt.plot([0, 1], [0, 1], 'k--', lw=2, label='Random Classifier')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('ROC Curves for Each Class')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('./outputs/roc_curves.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "# Generate plots if we have test results\n",
    "if len(test_labels) > 0:\n",
    "    plot_confusion_matrix(test_labels, test_preds, class_names)\n",
    "    \n",
    "    if len(test_probs) > 0 and len(test_probs[0]) == len(class_names):\n",
    "        plot_roc_curves(test_labels, test_probs, class_names)\n",
    "    else:\n",
    "        print(\"Skipping ROC curves due to insufficient probability data\")\nelse:\n",
    "    print(\"No test results available for plotting.\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4\ufe0f\u20e3 Explainability with Grad-CAM\n",
    "\n",
    "Let's use Grad-CAM (Gradient-weighted Class Activation Mapping) to visualize which parts of the retinal images our model focuses on when making predictions."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Grad-CAM implementation\n",
    "def get_gradcam_visualization(model, image, target_class, device):\n",
    "    \"\"\"Generate Grad-CAM visualization for a given image and target class\"\"\"\n",
    "    \n",
    "    # Define target layer (last convolutional layer of ResNet50)\n",
    "    target_layers = [model.backbone.layer4[-1]]\n",
    "    \n",
    "    # Initialize Grad-CAM\n",
    "    cam = GradCAM(model=model, target_layers=target_layers)\n",
    "    \n",
    "    # Generate CAM\n",
    "    targets = [ClassifierOutputTarget(target_class)]\n",
    "    \n",
    "    # Get gradcam output\n",
    "    grayscale_cam = cam(input_tensor=image.unsqueeze(0), targets=targets)\n",
    "    grayscale_cam = grayscale_cam[0, :]  # Remove batch dimension\n",
    "    \n",
    "    return grayscale_cam\n",
    "\n",
    "def visualize_gradcam_samples(model, dataset, device, num_samples=10):\n",
    "    \"\"\"Visualize Grad-CAM for sample images from each class\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Get samples from each class\n",
    "    samples_per_class = max(1, num_samples // len(class_names))\n",
    "    \n",
    "    fig, axes = plt.subplots(len(class_names), 3, figsize=(15, 3*len(class_names)))\n",
    "    fig.suptitle('Grad-CAM Visualizations by Class', fontsize=16)\n",
    "    \n",
    "    for class_id in range(len(class_names)):\n",
    "        # Find samples from this class\n",
    "        class_indices = [i for i, (_, label) in enumerate(dataset) if label == class_id]\n",
    "        \n",
    "        if len(class_indices) == 0:\n",
    "            # No samples for this class\n",
    "            for j in range(3):\n",
    "                axes[class_id, j].text(0.5, 0.5, f'No {class_names[class_id]} samples', \n",
    "                                     ha='center', va='center', transform=axes[class_id, j].transAxes)\n",
    "                axes[class_id, j].axis('off')\n",
    "            continue\n",
    "        \n",
    "        # Get a random sample\n",
    "        sample_idx = np.random.choice(class_indices)\n",
    "        image, label = dataset[sample_idx]\n",
    "        \n",
    "        # Move to device\n",
    "        image_tensor = image.to(device)\n",
    "        \n",
    "        # Get model prediction\n",
    "        with torch.no_grad():\n",
    "            output = model(image_tensor.unsqueeze(0))\n",
    "            probabilities = torch.softmax(output, dim=1)\n",
    "            predicted_class = torch.argmax(output, dim=1).item()\n",
    "            confidence = probabilities[0, predicted_class].item()\n",
    "        \n",
    "        try:\n",
    "            # Generate Grad-CAM\n",
    "            gradcam = get_gradcam_visualization(model, image_tensor, predicted_class, device)\n",
    "            \n",
    "            # Convert image to numpy for visualization\n",
    "            # Denormalize image\n",
    "            mean = np.array([0.485, 0.456, 0.406])\n",
    "            std = np.array([0.229, 0.224, 0.225])\n",
    "            \n",
    "            img_np = image.cpu().numpy().transpose(1, 2, 0)\n",
    "            img_np = std * img_np + mean\n",
    "            img_np = np.clip(img_np, 0, 1)\n",
    "            \n",
    "            # Create overlay\n",
    "            visualization = show_cam_on_image(img_np, gradcam, use_rgb=True)\n",
    "            \n",
    "            # Plot original image\n",
    "            axes[class_id, 0].imshow(img_np)\n",
    "            axes[class_id, 0].set_title(f'{class_names[class_id]}\\nOriginal')\n",
    "            axes[class_id, 0].axis('off')\n",
    "            \n",
    "            # Plot Grad-CAM heatmap\n",
    "            axes[class_id, 1].imshow(gradcam, cmap='jet')\n",
    "            axes[class_id, 1].set_title('Grad-CAM Heatmap')\n",
    "            axes[class_id, 1].axis('off')\n",
    "            \n",
    "            # Plot overlay\n",
    "            axes[class_id, 2].imshow(visualization)\n",
    "            axes[class_id, 2].set_title(f'Overlay\\nPred: {class_names[predicted_class]} ({confidence:.2f})')\n",
    "            axes[class_id, 2].axis('off')\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error generating Grad-CAM for class {class_names[class_id]}: {e}\")\n",
    "            # Show error message\n",
    "            for j in range(3):\n",
    "                axes[class_id, j].text(0.5, 0.5, f'Grad-CAM Error\\n{class_names[class_id]}', \n",
    "                                     ha='center', va='center', transform=axes[class_id, j].transAxes)\n",
    "                axes[class_id, j].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('./outputs/gradcam_visualizations.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "# Generate Grad-CAM visualizations\n",
    "print(\"Generating Grad-CAM visualizations...\")\n",
    "try:\n",
    "    visualize_gradcam_samples(model, test_dataset, device, num_samples=len(class_names))\n",
    "    print(\"\u2705 Grad-CAM visualizations generated successfully!\")\nexcept Exception as e:\n",
    "    print(f\"\u274c Error generating Grad-CAM visualizations: {e}\")\n",
    "    print(\"This might be due to missing images or model issues.\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5\ufe0f\u20e3 Model Export for Next.js Integration\n",
    "\n",
    "We'll export our trained model to ONNX format for use in a Next.js application with `onnxruntime-node`."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Export model to ONNX format\n",
    "def export_to_onnx(model, save_path, input_size=(1, 3, 224, 224)):\n",
    "    \"\"\"Export PyTorch model to ONNX format\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Create dummy input\n",
    "    dummy_input = torch.randn(input_size).to(device)\n",
    "    \n",
    "    # Export to ONNX\n",
    "    torch.onnx.export(\n",
    "        model,\n",
    "        dummy_input,\n",
    "        save_path,\n",
    "        export_params=True,\n",
    "        opset_version=11,\n",
    "        do_constant_folding=True,\n",
    "        input_names=['input'],\n",
    "        output_names=['output'],\n",
    "        dynamic_axes={\n",
    "            'input': {0: 'batch_size'},\n",
    "            'output': {0: 'batch_size'}\n",
    "        }\n",
    "    )\n",
    "\n",
    "def verify_onnx_model(onnx_path, pytorch_model, device):\n",
    "    \"\"\"Verify ONNX model produces same results as PyTorch model\"\"\"\n",
    "    # Load ONNX model\n",
    "    ort_session = ort.InferenceSession(onnx_path)\n",
    "    \n",
    "    # Create test input\n",
    "    test_input = torch.randn(1, 3, 224, 224).to(device)\n",
    "    \n",
    "    # PyTorch prediction\n",
    "    pytorch_model.eval()\n",
    "    with torch.no_grad():\n",
    "        pytorch_output = pytorch_model(test_input).cpu().numpy()\n",
    "    \n",
    "    # ONNX prediction\n",
    "    ort_inputs = {ort_session.get_inputs()[0].name: test_input.cpu().numpy()}\n",
    "    ort_output = ort_session.run(None, ort_inputs)[0]\n",
    "    \n",
    "    # Compare outputs\n",
    "    max_diff = np.max(np.abs(pytorch_output - ort_output))\n",
    "    \n",
    "    return max_diff < 1e-5, max_diff\n",
    "\n",
    "# Export model\n",
    "print(\"Exporting model to ONNX format...\")\n",
    "onnx_path = './outputs/diabetic_retinopathy_model.onnx'\n",
    "\n",
    "try:\n",
    "    export_to_onnx(model, onnx_path)\n",
    "    print(f\"\u2705 Model exported to: {onnx_path}\")\n",
    "    \n",
    "    # Verify ONNX model\n",
    "    is_valid, max_diff = verify_onnx_model(onnx_path, model, device)\n",
    "    \n",
    "    if is_valid:\n",
    "        print(f\"\u2705 ONNX model verification successful (max diff: {max_diff:.2e})\")\n",
    "    else:\n",
    "        print(f\"\u26a0\ufe0f  ONNX model verification failed (max diff: {max_diff:.2e})\")\n",
    "    \n",
    "    # Get model size\n",
    "    import os\n",
    "    model_size_mb = os.path.getsize(onnx_path) / (1024 * 1024)\n",
    "    print(f\"Model size: {model_size_mb:.2f} MB\")\n",
    "    \nexcept Exception as e:\n",
    "    print(f\"\u274c Error exporting to ONNX: {e}\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Export preprocessing parameters for frontend\n",
    "preprocessing_config = {\n",
    "    'image_size': CONFIG['IMAGE_SIZE'],\n",
    "    'mean': [0.485, 0.456, 0.406],\n",
    "    'std': [0.229, 0.224, 0.225],\n",
    "    'class_names': class_names,\n",
    "    'num_classes': CONFIG['NUM_CLASSES'],\n",
    "    'model_architecture': CONFIG['MODEL_NAME'],\n",
    "    'input_shape': [1, 3, CONFIG['IMAGE_SIZE'], CONFIG['IMAGE_SIZE']],\n",
    "    'output_shape': [1, CONFIG['NUM_CLASSES']]\n",
    "}\n",
    "\n",
    "# Save preprocessing config\n",
    "with open('./outputs/preprocessing_config.json', 'w') as f:\n",
    "    json.dump(preprocessing_config, f, indent=2)\n",
    "\n",
    "print(\"Preprocessing configuration saved to: ./outputs/preprocessing_config.json\")\n",
    "print(\"\\nPreprocessing Config:\")\n",
    "for key, value in preprocessing_config.items():\n",
    "    print(f\"{key}: {value}\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Optional: Quantize ONNX model to reduce size\n",
    "def quantize_onnx_model(input_path, output_path):\n",
    "    \"\"\"Quantize ONNX model to reduce size\"\"\"\n",
    "    try:\n",
    "        from onnxruntime.quantization import quantize_dynamic, QuantType\n",
    "        \n",
    "        quantize_dynamic(\n",
    "            input_path,\n",
    "            output_path,\n",
    "            weight_type=QuantType.QUInt8\n",
    "        )\n",
    "        \n",
    "        return True\n",
    "    except ImportError:\n",
    "        print(\"ONNX quantization not available. Install with: pip install onnxruntime-tools\")\n",
    "        return False\n",
    "    except Exception as e:\n",
    "        print(f\"Quantization error: {e}\")\n",
    "        return False\n",
    "\n",
    "# Quantize model\n",
    "print(\"\\nAttempting to quantize ONNX model...\")\n",
    "quantized_path = './outputs/diabetic_retinopathy_model_quantized.onnx'\n",
    "\n",
    "if os.path.exists(onnx_path):\n",
    "    success = quantize_onnx_model(onnx_path, quantized_path)\n",
    "    \n",
    "    if success and os.path.exists(quantized_path):\n",
    "        original_size = os.path.getsize(onnx_path) / (1024 * 1024)\n",
    "        quantized_size = os.path.getsize(quantized_path) / (1024 * 1024)\n",
    "        compression_ratio = original_size / quantized_size\n",
    "        \n",
    "        print(f\"\u2705 Quantized model saved to: {quantized_path}\")\n",
    "        print(f\"Original size: {original_size:.2f} MB\")\n",
    "        print(f\"Quantized size: {quantized_size:.2f} MB\")\n",
    "        print(f\"Compression ratio: {compression_ratio:.2f}x\")\n",
    "    else:\n",
    "        print(\"\u274c Quantization failed or not available\")\nelse:\n",
    "    print(\"No ONNX model found for quantization\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7\ufe0f\u20e3 Next.js Integration Guide\n",
    "\n",
    "Here's how to integrate the exported ONNX model into your Next.js application:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Generate Next.js integration code\n",
    "nextjs_integration_code = '''\n",
    "// Next.js API Route Example: /pages/api/predict.js or /app/api/predict/route.js\n",
    "\n",
    "import * as ort from 'onnxruntime-node';\n",
    "import sharp from 'sharp';\n",
    "import path from 'path';\n",
    "import fs from 'fs';\n",
    "\n",
    "// Load preprocessing config\n",
    "const configPath = path.join(process.cwd(), 'models', 'preprocessing_config.json');\n",
    "const config = JSON.parse(fs.readFileSync(configPath, 'utf8'));\n",
    "\n",
    "// Load ONNX model\n",
    "let session = null;\n",
    "\n",
    "async function loadModel() {\n",
    "  if (!session) {\n",
    "    const modelPath = path.join(process.cwd(), 'models', 'diabetic_retinopathy_model.onnx');\n",
    "    session = await ort.InferenceSession.create(modelPath);\n",
    "  }\n",
    "  return session;\n",
    "}\n",
    "\n",
    "// Preprocess image\n",
    "async function preprocessImage(imageBuffer) {\n",
    "  // Resize and normalize image\n",
    "  const { data, info } = await sharp(imageBuffer)\n",
    "    .resize(config.image_size, config.image_size)\n",
    "    .raw()\n",
    "    .toBuffer({ resolveWithObject: true });\n",
    "  \n",
    "  // Convert to float32 and normalize\n",
    "  const pixels = new Float32Array(data.length);\n",
    "  \n",
    "  for (let i = 0; i < data.length; i += 3) {\n",
    "    // Normalize RGB channels\n",
    "    pixels[i] = (data[i] / 255.0 - config.mean[0]) / config.std[0];     // R\n",
    "    pixels[i + 1] = (data[i + 1] / 255.0 - config.mean[1]) / config.std[1]; // G\n",
    "    pixels[i + 2] = (data[i + 2] / 255.0 - config.mean[2]) / config.std[2]; // B\n",
    "  }\n",
    "  \n",
    "  // Reshape to [1, 3, 224, 224] format\n",
    "  const tensor = new Float32Array(1 * 3 * config.image_size * config.image_size);\n",
    "  \n",
    "  for (let c = 0; c < 3; c++) {\n",
    "    for (let h = 0; h < config.image_size; h++) {\n",
    "      for (let w = 0; w < config.image_size; w++) {\n",
    "        const pixelIndex = (h * config.image_size + w) * 3 + c;\n",
    "        const tensorIndex = c * config.image_size * config.image_size + h * config.image_size + w;\n",
    "        tensor[tensorIndex] = pixels[pixelIndex];\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "  \n",
    "  return tensor;\n",
    "}\n",
    "\n",
    "// API handler\n",
    "export default async function handler(req, res) {\n",
    "  if (req.method !== 'POST') {\n",
    "    return res.status(405).json({ error: 'Method not allowed' });\n",
    "  }\n",
    "  \n",
    "  try {\n",
    "    // Get image from request\n",
    "    const { image } = req.body; // Base64 encoded image\n",
    "    const imageBuffer = Buffer.from(image, 'base64');\n",
    "    \n",
    "    // Preprocess image\n",
    "    const inputTensor = await preprocessImage(imageBuffer);\n",
    "    \n",
    "    // Load model and run inference\n",
    "    const session = await loadModel();\n",
    "    const feeds = { input: new ort.Tensor('float32', inputTensor, [1, 3, 224, 224]) };\n",
    "    const results = await session.run(feeds);\n",
    "    \n",
    "    // Get predictions\n",
    "    const output = results.output.data;\n",
    "    \n",
    "    // Apply softmax to get probabilities\n",
    "    const maxLogit = Math.max(...output);\n",
    "    const expLogits = output.map(x => Math.exp(x - maxLogit));\n",
    "    const sumExp = expLogits.reduce((a, b) => a + b, 0);\n",
    "    const probabilities = expLogits.map(x => x / sumExp);\n",
    "    \n",
    "    // Get predicted class\n",
    "    const predictedClass = probabilities.indexOf(Math.max(...probabilities));\n",
    "    const confidence = probabilities[predictedClass];\n",
    "    \n",
    "    // Prepare response\n",
    "    const response = {\n",
    "      predicted_class: predictedClass,\n",
    "      predicted_label: config.class_names[predictedClass],\n",
    "      confidence: confidence,\n",
    "      probabilities: probabilities.reduce((acc, prob, idx) => {\n",
    "        acc[config.class_names[idx]] = prob;\n",
    "        return acc;\n",
    "      }, {})\n",
    "    };\n",
    "    \n",
    "    res.status(200).json(response);\n",
    "    \n",
    "  } catch (error) {\n",
    "    console.error('Prediction error:', error);\n",
    "    res.status(500).json({ error: 'Prediction failed', details: error.message });\n",
    "  }\n",
    "}\n",
    "'''\n",
    "\n",
    "# Save Next.js integration code\n",
    "with open('./outputs/nextjs_integration.js', 'w') as f:\n",
    "    f.write(nextjs_integration_code.strip())\n",
    "\n",
    "print(\"Next.js integration code saved to: ./outputs/nextjs_integration.js\")\n",
    "print(\"\\nIntegration Steps:\")\n",
    "print(\"1. Copy the ONNX model and preprocessing config to your Next.js project\")\n",
    "print(\"2. Install dependencies: npm install onnxruntime-node sharp\")\n",
    "print(\"3. Create the API route using the provided code\")\n",
    "print(\"4. Test the API with retinal images\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \ud83d\udcca Final Model Performance Summary\n",
    "\n",
    "Let's summarize the final model performance and provide recommendations."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Create comprehensive model report\n",
    "def create_model_report():\n",
    "    \"\"\"Create a comprehensive model performance report\"\"\"\n",
    "    \n",
    "    report = {\n",
    "        'model_info': {\n",
    "            'architecture': CONFIG['MODEL_NAME'],\n",
    "            'num_classes': CONFIG['NUM_CLASSES'],\n",
    "            'input_size': f\"{CONFIG['IMAGE_SIZE']}x{CONFIG['IMAGE_SIZE']}\",\n",
    "            'total_parameters': sum(p.numel() for p in model.parameters()),\n",
    "            'trainable_parameters': model.get_trainable_params()\n",
    "        },\n",
    "        'training_info': {\n",
    "            'total_epochs': len(history['train_loss']) if history['train_loss'] else 0,\n",
    "            'phase1_epochs': phase1_epochs if 'phase1_epochs' in locals() else 0,\n",
    "            'phase2_epochs': phase2_epochs if 'phase2_epochs' in locals() else 0,\n",
    "            'batch_size': CONFIG['BATCH_SIZE'],\n",
    "            'learning_rate': CONFIG['LEARNING_RATE']\n",
    "        },\n",
    "        'dataset_info': {\n",
    "            'total_samples': len(labels_final) if 'labels_final' in locals() else 0,\n",
    "            'train_samples': len(train_df) if 'train_df' in locals() else 0,\n",
    "            'val_samples': len(val_df) if 'val_df' in locals() else 0,\n",
    "            'test_samples': len(test_df) if 'test_df' in locals() else 0,\n",
    "            'class_distribution': dict(labels_final['label_clean'].value_counts().sort_index()) if 'labels_final' in locals() else {}\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Add performance metrics if available\n",
    "    if 'test_metrics' in locals() and test_metrics:\n",
    "        report['performance'] = {\n",
    "            'test_accuracy': test_metrics['accuracy'],\n",
    "            'test_precision': test_metrics['precision'],\n",
    "            'test_recall': test_metrics['recall'],\n",
    "            'test_f1': test_metrics['f1'],\n",
    "            'test_auc': test_metrics.get('auc', 0.0)\n",
    "        }\n",
    "    \n",
    "    # Add training history if available\n",
    "    if history['train_loss']:\n",
    "        report['training_history'] = {\n",
    "            'best_val_loss': min(history['val_loss']),\n",
    "            'best_val_accuracy': max(history['val_acc']),\n",
    "            'best_val_f1': max(history['val_f1']),\n",
    "            'best_val_auc': max(history['val_auc'])\n",
    "        }\n",
    "    \n",
    "    return report\n",
    "\n",
    "# Generate report\n",
    "model_report = create_model_report()\n",
    "\n",
    "# Save report\n",
    "with open('./outputs/model_report.json', 'w') as f:\n",
    "    json.dump(model_report, f, indent=2)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"DIABETIC RETINOPATHY DETECTION MODEL REPORT\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\\n\ud83d\udccb Model Information:\")\n",
    "for key, value in model_report['model_info'].items():\n",
    "    print(f\"  {key.replace('_', ' ').title()}: {value:,}\" if isinstance(value, int) else f\"  {key.replace('_', ' ').title()}: {value}\")\n",
    "\n",
    "print(f\"\\n\ud83c\udfaf Training Information:\")\n",
    "for key, value in model_report['training_info'].items():\n",
    "    print(f\"  {key.replace('_', ' ').title()}: {value}\")\n",
    "\n",
    "print(f\"\\n\ud83d\udcca Dataset Information:\")\n",
    "for key, value in model_report['dataset_info'].items():\n",
    "    if key != 'class_distribution':\n",
    "        print(f\"  {key.replace('_', ' ').title()}: {value}\")\n",
    "\n",
    "if 'class_distribution' in model_report['dataset_info'] and model_report['dataset_info']['class_distribution']:\n",
    "    print(f\"\\n  Class Distribution:\")\n",
    "    for class_id, count in model_report['dataset_info']['class_distribution'].items():\n",
    "        if class_id < len(class_names):\n",
    "            print(f\"    {class_names[class_id]}: {count}\")\n",
    "\n",
    "if 'performance' in model_report:\n",
    "    print(f\"\\n\ud83c\udfc6 Test Performance:\")\n",
    "    for key, value in model_report['performance'].items():\n",
    "        print(f\"  {key.replace('_', ' ').title()}: {value:.4f}\")\n",
    "\n",
    "if 'training_history' in model_report:\n",
    "    print(f\"\\n\ud83d\udcc8 Best Training Metrics:\")\n",
    "    for key, value in model_report['training_history'].items():\n",
    "        print(f\"  {key.replace('_', ' ').title()}: {value:.4f}\")\n",
    "\n",
    "print(f\"\\n\ud83d\udcbe Exported Files:\")\n",
    "exported_files = [\n",
    "    './outputs/diabetic_retinopathy_model.onnx',\n",
    "    './outputs/preprocessing_config.json',\n",
    "    './outputs/nextjs_integration.js',\n",
    "    './outputs/model_report.json'\n",
    "]\n",
    "\n",
    "for file_path in exported_files:\n",
    "    if os.path.exists(file_path):\n",
    "        file_size = os.path.getsize(file_path) / 1024  # KB\n",
    "        print(f\"  \u2705 {file_path} ({file_size:.1f} KB)\")\n",
    "    else:\n",
    "        print(f\"  \u274c {file_path} (not found)\")\n",
    "\n",
    "print(f\"\\n\ud83d\ude80 Ready for deployment!\")\n",
    "print(\"Model report saved to: ./outputs/model_report.json\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \ud83d\ude80 Deployment Recommendations\n",
    "\n",
    "### Model Performance Considerations\n",
    "\n",
    "1. **Sensitivity vs Specificity**: For medical applications, consider the trade-off between false positives and false negatives\n",
    "2. **Confidence Thresholds**: Implement confidence-based decision making\n",
    "3. **Edge Cases**: Handle low-quality images and edge cases gracefully\n",
    "\n",
    "### Production Deployment\n",
    "\n",
    "1. **Model Serving**: Use the exported ONNX model with onnxruntime-node\n",
    "2. **Image Processing**: Ensure consistent preprocessing pipeline\n",
    "3. **Error Handling**: Implement robust error handling for various image formats\n",
    "4. **Monitoring**: Track prediction confidence and model performance over time\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "1. **Clinical Validation**: Validate model performance with medical professionals\n",
    "2. **Regulatory Compliance**: Ensure compliance with medical device regulations\n",
    "3. **Continuous Learning**: Implement feedback loops for model improvement\n",
    "4. **A/B Testing**: Compare model versions in production\n",
    "\n",
    "### Files Generated\n",
    "\n",
    "- `diabetic_retinopathy_model.onnx`: Main model for inference\n",
    "- `preprocessing_config.json`: Preprocessing parameters\n",
    "- `nextjs_integration.js`: Next.js API route example\n",
    "- `model_report.json`: Comprehensive model report\n",
    "- Various visualization images (training history, confusion matrix, ROC curves, Grad-CAM)\n",
    "\n",
    "**\ud83c\udf89 Congratulations! Your diabetic retinopathy detection pipeline is complete and ready for integration!**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}