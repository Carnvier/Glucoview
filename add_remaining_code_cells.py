#!/usr/bin/env python3
"""
Script to add remaining code cells to the code-only notebook
"""

import json

def create_cell(cell_type, source, metadata=None):
    """Create a notebook cell"""
    cell = {
        "cell_type": cell_type,
        "metadata": metadata or {},
        "source": source if isinstance(source, list) else [source]
    }
    
    if cell_type == "code":
        cell["execution_count"] = None
        cell["outputs"] = []
    
    return cell

# Load existing notebook
with open('/workspace/diabetic_retinopathy_code_only.ipynb', 'r') as f:
    notebook = json.load(f)

# Additional code cells
additional_cells = []

# Data loaders
additional_cells.append(create_cell("code", [
    "# ============================================================================\n",
    "# DATA LOADERS WITH WEIGHTED SAMPLING\n",
    "# ============================================================================\n",
    "# Create data loaders with weighted sampling to handle class imbalance\n",
    "\n",
    "def create_weighted_sampler(dataset, labels):\n",
    "    \"\"\"\n",
    "    Create weighted sampler for imbalanced dataset\n",
    "    This ensures balanced sampling during training\n",
    "    \"\"\"\n",
    "    class_counts = np.bincount(labels)\n",
    "    class_weights = 1.0 / class_counts\n",
    "    sample_weights = [class_weights[label] for label in labels]\n",
    "    \n",
    "    return WeightedRandomSampler(\n",
    "        weights=sample_weights,\n",
    "        num_samples=len(sample_weights),\n",
    "        replacement=True\n",
    "    )\n",
    "\n",
    "print(\"üîÑ Creating data loaders with weighted sampling...\")\n",
    "\n",
    "# Create weighted sampler for training to handle class imbalance\n",
    "train_sampler = create_weighted_sampler(train_dataset, train_df['label_clean'].values)\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=CONFIG['BATCH_SIZE'],\n",
    "    sampler=train_sampler,  # Use weighted sampler instead of shuffle\n",
    "    num_workers=4,\n",
    "    pin_memory=True,\n",
    "    drop_last=True\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=CONFIG['BATCH_SIZE'],\n",
    "    shuffle=False,\n",
    "    num_workers=4,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=CONFIG['BATCH_SIZE'],\n",
    "    shuffle=False,\n",
    "    num_workers=4,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "print(f\"üìä Data loaders created:\")\n",
    "print(f\"  Train batches: {len(train_loader)}\")\n",
    "print(f\"  Validation batches: {len(val_loader)}\")\n",
    "print(f\"  Test batches: {len(test_loader)}\")\n",
    "\n",
    "# Test data loading\n",
    "print(\"\\nüß™ Testing data loading...\")\n",
    "try:\n",
    "    sample_batch = next(iter(train_loader))\n",
    "    images, labels = sample_batch\n",
    "    print(f\"‚úÖ Batch shape: {images.shape}\")\n",
    "    print(f\"‚úÖ Labels shape: {labels.shape}\")\n",
    "    print(f\"‚úÖ Image range: [{images.min():.3f}, {images.max():.3f}]\")\n",
    "    print(f\"‚úÖ Sample labels: {labels[:10].tolist()}\")\n",
    "    print(\"‚úÖ Data loading successful!\")\nexcept Exception as e:\n",
    "    print(f\"‚ùå Data loading error: {e}\")\n",
    "    print(\"üìù Note: This is expected if no images are available.\")\n",
    "    print(\"üìù The model training will still work with dummy data.\")\n",
    "\n",
    "print(\"\\n‚úÖ Data loaders setup completed!\")"
]))

# Model architecture
additional_cells.append(create_cell("code", [
    "# ============================================================================\n",
    "# RESNET50 MODEL ARCHITECTURE\n",
    "# ============================================================================\n",
    "# Custom ResNet50 model with two-phase training capability\n",
    "\n",
    "class DiabeticRetinopathyModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Custom ResNet50 model for diabetic retinopathy detection\n",
    "    \n",
    "    Features:\n",
    "    - Pre-trained ResNet50 backbone\n",
    "    - Custom classifier head with dropout and batch normalization\n",
    "    - Freeze/unfreeze capability for two-phase training\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, num_classes=5, model_name='resnet50', pretrained=True):\n",
    "        super(DiabeticRetinopathyModel, self).__init__()\n",
    "        \n",
    "        # Load pretrained backbone\n",
    "        if model_name == 'resnet50':\n",
    "            self.backbone = models.resnet50(pretrained=pretrained)\n",
    "            num_features = self.backbone.fc.in_features\n",
    "            self.backbone.fc = nn.Identity()  # Remove final layer\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported model: {model_name}\")\n",
    "        \n",
    "        # Custom classifier head with regularization\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(num_features, 512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(512, num_classes)\n",
    "        )\n",
    "        \n",
    "        self.num_classes = num_classes\n",
    "        self.model_name = model_name\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Extract features from backbone\n",
    "        features = self.backbone(x)\n",
    "        \n",
    "        # Classify using custom head\n",
    "        output = self.classifier(features)\n",
    "        \n",
    "        return output\n",
    "    \n",
    "    def freeze_backbone(self):\n",
    "        \"\"\"Freeze backbone parameters for Phase 1 training\"\"\"\n",
    "        for param in self.backbone.parameters():\n",
    "            param.requires_grad = False\n",
    "        print(\"üßä Backbone frozen for Phase 1 training\")\n",
    "    \n",
    "    def unfreeze_backbone(self):\n",
    "        \"\"\"Unfreeze backbone parameters for Phase 2 training\"\"\"\n",
    "        for param in self.backbone.parameters():\n",
    "            param.requires_grad = True\n",
    "        print(\"üî• Backbone unfrozen for Phase 2 training\")\n",
    "    \n",
    "    def get_trainable_params(self):\n",
    "        \"\"\"Get number of trainable parameters\"\"\"\n",
    "        return sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
    "\n",
    "# Create model instance\n",
    "print(\"üèóÔ∏è  Creating ResNet50 model...\")\n",
    "model = DiabeticRetinopathyModel(\n",
    "    num_classes=CONFIG['NUM_CLASSES'],\n",
    "    model_name=CONFIG['MODEL_NAME'],\n",
    "    pretrained=True\n",
    ").to(device)\n",
    "\n",
    "print(f\"\\nüìä Model Information:\")\n",
    "print(f\"  Architecture: {CONFIG['MODEL_NAME']}\")\n",
    "print(f\"  Total parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "print(f\"  Trainable parameters: {model.get_trainable_params():,}\")\n",
    "print(f\"  Model size: ~{sum(p.numel() for p in model.parameters()) * 4 / 1024**2:.1f} MB\")\n",
    "\n",
    "# Test model forward pass\n",
    "print(\"\\nüß™ Testing model forward pass...\")\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    dummy_input = torch.randn(2, 3, CONFIG['IMAGE_SIZE'], CONFIG['IMAGE_SIZE']).to(device)\n",
    "    dummy_output = model(dummy_input)\n",
    "    print(f\"‚úÖ Input shape: {dummy_input.shape}\")\n",
    "    print(f\"‚úÖ Output shape: {dummy_output.shape}\")\n",
    "    print(f\"‚úÖ Output range: [{dummy_output.min():.3f}, {dummy_output.max():.3f}]\")\n",
    "    print(\"‚úÖ Model forward pass successful!\")\n",
    "\n",
    "print(\"\\n‚úÖ Model architecture setup completed!\")"
]))

# Training utilities
additional_cells.append(create_cell("code", [
    "# ============================================================================\n",
    "# TRAINING UTILITIES AND HELPER FUNCTIONS\n",
    "# ============================================================================\n",
    "# Comprehensive training utilities for model training and evaluation\n",
    "\n",
    "class EarlyStopping:\n",
    "    \"\"\"\n",
    "    Early stopping implementation to prevent overfitting\n",
    "    \"\"\"\n",
    "    def __init__(self, patience=10, min_delta=0.001, restore_best_weights=True):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.restore_best_weights = restore_best_weights\n",
    "        self.best_loss = None\n",
    "        self.counter = 0\n",
    "        self.best_weights = None\n",
    "    \n",
    "    def __call__(self, val_loss, model):\n",
    "        if self.best_loss is None:\n",
    "            self.best_loss = val_loss\n",
    "            self.save_checkpoint(model)\n",
    "        elif val_loss < self.best_loss - self.min_delta:\n",
    "            self.best_loss = val_loss\n",
    "            self.counter = 0\n",
    "            self.save_checkpoint(model)\n",
    "        else:\n",
    "            self.counter += 1\n",
    "        \n",
    "        if self.counter >= self.patience:\n",
    "            if self.restore_best_weights:\n",
    "                model.load_state_dict(self.best_weights)\n",
    "            return True\n",
    "        return False\n",
    "    \n",
    "    def save_checkpoint(self, model):\n",
    "        self.best_weights = model.state_dict().copy()\n",
    "\n",
    "def calculate_metrics(y_true, y_pred, y_prob=None):\n",
    "    \"\"\"\n",
    "    Calculate comprehensive evaluation metrics\n",
    "    \"\"\"\n",
    "    from sklearn.metrics import (\n",
    "        accuracy_score, precision_recall_fscore_support,\n",
    "        roc_auc_score, confusion_matrix\n",
    "    )\n",
    "    \n",
    "    # Basic metrics\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(y_true, y_pred, average='weighted', zero_division=0)\n",
    "    \n",
    "    metrics = {\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1\n",
    "    }\n",
    "    \n",
    "    # Calculate AUC if probabilities provided\n",
    "    if y_prob is not None:\n",
    "        try:\n",
    "            auc_score = roc_auc_score(y_true, y_prob, multi_class='ovr', average='weighted')\n",
    "            metrics['auc'] = auc_score\n",
    "        except Exception:\n",
    "            metrics['auc'] = 0.0\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "def train_epoch(model, loader, criterion, optimizer, device, epoch=0):\n",
    "    \"\"\"\n",
    "    Train model for one epoch\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    all_probs = []\n",
    "    \n",
    "    # Progress tracking\n",
    "    batch_count = 0\n",
    "    total_batches = len(loader)\n",
    "    \n",
    "    for batch_idx, (images, labels) in enumerate(loader):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Track metrics\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        # Get predictions\n",
    "        probs = torch.softmax(outputs, dim=1)\n",
    "        preds = torch.argmax(outputs, dim=1)\n",
    "        \n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "        all_probs.extend(probs.cpu().numpy())\n",
    "        \n",
    "        batch_count += 1\n",
    "        \n",
    "        # Print progress every 10% of batches\n",
    "        if batch_count % max(1, total_batches // 10) == 0:\n",
    "            progress = batch_count / total_batches * 100\n",
    "            print(f\"  Training progress: {progress:.0f}% | Loss: {loss.item():.4f}\")\n",
    "    \n",
    "    # Calculate epoch metrics\n",
    "    epoch_loss = running_loss / len(loader)\n",
    "    metrics = calculate_metrics(all_labels, all_preds, all_probs)\n",
    "    \n",
    "    return epoch_loss, metrics\n",
    "\n",
    "def validate_epoch(model, loader, criterion, device):\n",
    "    \"\"\"\n",
    "    Validate model for one epoch\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    all_probs = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            \n",
    "            # Get predictions\n",
    "            probs = torch.softmax(outputs, dim=1)\n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "            \n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_probs.extend(probs.cpu().numpy())\n",
    "    \n",
    "    # Calculate epoch metrics\n",
    "    epoch_loss = running_loss / len(loader)\n",
    "    metrics = calculate_metrics(all_labels, all_preds, all_probs)\n",
    "    \n",
    "    return epoch_loss, metrics, all_labels, all_preds, all_probs\n",
    "\n",
    "print(\"‚úÖ Training utilities defined successfully!\")\n",
    "print(\"üõ†Ô∏è  Available utilities:\")\n",
    "print(\"  - EarlyStopping: Prevent overfitting\")\n",
    "print(\"  - calculate_metrics: Comprehensive evaluation\")\n",
    "print(\"  - train_epoch: Training loop\")\n",
    "print(\"  - validate_epoch: Validation loop\")"
]))

# Phase 1 training
additional_cells.append(create_cell("code", [
    "# ============================================================================\n",
    "# PHASE 1 TRAINING: FROZEN BACKBONE\n",
    "# ============================================================================\n",
    "# Train only the classifier head while keeping ResNet50 backbone frozen\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"üéØ PHASE 1: TRAINING CLASSIFIER HEAD (FROZEN BACKBONE)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Freeze backbone for Phase 1\n",
    "model.freeze_backbone()\n",
    "print(f\"üìä Trainable parameters: {model.get_trainable_params():,}\")\n",
    "\n",
    "# Setup training components\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights_tensor)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=CONFIG['LEARNING_RATE'], weight_decay=CONFIG['WEIGHT_DECAY'])\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5, verbose=True)\n",
    "early_stopping = EarlyStopping(patience=CONFIG['PATIENCE'], min_delta=CONFIG['MIN_DELTA'])\n",
    "\n",
    "# Training history tracking\n",
    "history = {\n",
    "    'train_loss': [], 'val_loss': [],\n",
    "    'train_acc': [], 'val_acc': [],\n",
    "    'train_f1': [], 'val_f1': [],\n",
    "    'val_auc': []\n",
    "}\n",
    "\n",
    "print(f\"\\n‚öôÔ∏è  Training Configuration:\")\n",
    "print(f\"  Learning rate: {CONFIG['LEARNING_RATE']}\")\n",
    "print(f\"  Batch size: {CONFIG['BATCH_SIZE']}\")\n",
    "print(f\"  Max epochs: {CONFIG['NUM_EPOCHS']}\")\n",
    "print(f\"  Early stopping patience: {CONFIG['PATIENCE']}\")\n",
    "print(f\"  Weight decay: {CONFIG['WEIGHT_DECAY']}\")\n",
    "\n",
    "print(f\"\\nüöÄ Starting Phase 1 training...\")\n",
    "best_val_loss = float('inf')\n",
    "phase1_epochs = 0\n",
    "\n",
    "for epoch in range(CONFIG['NUM_EPOCHS']):\n",
    "    print(f\"\\nüìÖ Epoch {epoch+1}/{CONFIG['NUM_EPOCHS']}\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # Training phase\n",
    "    print(\"üèÉ Training...\")\n",
    "    train_loss, train_metrics = train_epoch(model, train_loader, criterion, optimizer, device, epoch)\n",
    "    \n",
    "    # Validation phase\n",
    "    print(\"üîç Validating...\")\n",
    "    val_loss, val_metrics, _, _, _ = validate_epoch(model, val_loader, criterion, device)\n",
    "    \n",
    "    # Update learning rate\n",
    "    scheduler.step(val_loss)\n",
    "    current_lr = optimizer.param_groups[0]['lr']\n",
    "    \n",
    "    # Store history\n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['val_loss'].append(val_loss)\n",
    "    history['train_acc'].append(train_metrics['accuracy'])\n",
    "    history['val_acc'].append(val_metrics['accuracy'])\n",
    "    history['train_f1'].append(train_metrics['f1'])\n",
    "    history['val_f1'].append(val_metrics['f1'])\n",
    "    history['val_auc'].append(val_metrics.get('auc', 0.0))\n",
    "    \n",
    "    # Print epoch results\n",
    "    print(f\"\\nüìä Epoch {epoch+1} Results:\")\n",
    "    print(f\"  Train ‚Üí Loss: {train_loss:.4f} | Acc: {train_metrics['accuracy']:.4f} | F1: {train_metrics['f1']:.4f}\")\n",
    "    print(f\"  Val   ‚Üí Loss: {val_loss:.4f} | Acc: {val_metrics['accuracy']:.4f} | F1: {val_metrics['f1']:.4f} | AUC: {val_metrics.get('auc', 0.0):.4f}\")\n",
    "    print(f\"  LR: {current_lr:.2e}\")\n",
    "    \n",
    "    # Early stopping check\n",
    "    if early_stopping(val_loss, model):\n",
    "        print(f\"\\n‚èπÔ∏è  Early stopping triggered after {epoch+1} epochs\")\n",
    "        break\n",
    "    \n",
    "    # Save best model\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'val_loss': val_loss,\n",
    "            'val_metrics': val_metrics,\n",
    "            'config': CONFIG\n",
    "        }, os.path.join(CONFIG['SAVE_PATH'], 'best_phase1_model.pth'))\n",
    "        print(f\"üíæ Best model saved (Val Loss: {val_loss:.4f})\")\n",
    "    \n",
    "    phase1_epochs = epoch + 1\n",
    "\n",
    "print(f\"\\nüèÅ Phase 1 completed after {phase1_epochs} epochs\")\n",
    "print(f\"üèÜ Best validation loss: {best_val_loss:.4f}\")\n",
    "print(f\"üìà Final train accuracy: {history['train_acc'][-1]:.4f}\")\n",
    "print(f\"üìà Final validation accuracy: {history['val_acc'][-1]:.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"‚úÖ PHASE 1 TRAINING COMPLETED SUCCESSFULLY!\")\n",
    "print(\"=\"*70)"
]))

# Phase 2 training
additional_cells.append(create_cell("code", [
    "# ============================================================================\n",
    "# PHASE 2 TRAINING: UNFROZEN BACKBONE (FINE-TUNING)\n",
    "# ============================================================================\n",
    "# Fine-tune the entire model with a smaller learning rate\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"üî• PHASE 2: FINE-TUNING ENTIRE MODEL (UNFROZEN BACKBONE)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Unfreeze backbone for Phase 2\n",
    "model.unfreeze_backbone()\n",
    "print(f\"üìä Trainable parameters: {model.get_trainable_params():,}\")\n",
    "\n",
    "# Setup training with smaller learning rate for fine-tuning\n",
    "fine_tune_lr = CONFIG['LEARNING_RATE'] / 10  # 10x smaller LR for fine-tuning\n",
    "optimizer = optim.AdamW(model.parameters(), lr=fine_tune_lr, weight_decay=CONFIG['WEIGHT_DECAY'])\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3, verbose=True)\n",
    "early_stopping = EarlyStopping(patience=CONFIG['PATIENCE']//2, min_delta=CONFIG['MIN_DELTA']/2)  # More sensitive\n",
    "\n",
    "print(f\"\\n‚öôÔ∏è  Phase 2 Configuration:\")\n",
    "print(f\"  Fine-tuning learning rate: {fine_tune_lr}\")\n",
    "print(f\"  Max epochs: {CONFIG['NUM_EPOCHS']//2}\")\n",
    "print(f\"  Early stopping patience: {CONFIG['PATIENCE']//2}\")\n",
    "\n",
    "print(f\"\\nüöÄ Starting Phase 2 fine-tuning...\")\n",
    "phase2_epochs = 0\n",
    "best_val_loss_phase2 = float('inf')\n",
    "\n",
    "for epoch in range(CONFIG['NUM_EPOCHS']//2):  # Fewer epochs for fine-tuning\n",
    "    print(f\"\\nüìÖ Phase 2 - Epoch {epoch+1}/{CONFIG['NUM_EPOCHS']//2}\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # Training phase\n",
    "    print(\"üèÉ Fine-tuning...\")\n",
    "    train_loss, train_metrics = train_epoch(model, train_loader, criterion, optimizer, device, epoch)\n",
    "    \n",
    "    # Validation phase\n",
    "    print(\"üîç Validating...\")\n",
    "    val_loss, val_metrics, _, _, _ = validate_epoch(model, val_loader, criterion, device)\n",
    "    \n",
    "    # Update learning rate\n",
    "    scheduler.step(val_loss)\n",
    "    current_lr = optimizer.param_groups[0]['lr']\n",
    "    \n",
    "    # Store history\n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['val_loss'].append(val_loss)\n",
    "    history['train_acc'].append(train_metrics['accuracy'])\n",
    "    history['val_acc'].append(val_metrics['accuracy'])\n",
    "    history['train_f1'].append(train_metrics['f1'])\n",
    "    history['val_f1'].append(val_metrics['f1'])\n",
    "    history['val_auc'].append(val_metrics.get('auc', 0.0))\n",
    "    \n",
    "    # Print epoch results\n",
    "    print(f\"\\nüìä Phase 2 Epoch {epoch+1} Results:\")\n",
    "    print(f\"  Train ‚Üí Loss: {train_loss:.4f} | Acc: {train_metrics['accuracy']:.4f} | F1: {train_metrics['f1']:.4f}\")\n",
    "    print(f\"  Val   ‚Üí Loss: {val_loss:.4f} | Acc: {val_metrics['accuracy']:.4f} | F1: {val_metrics['f1']:.4f} | AUC: {val_metrics.get('auc', 0.0):.4f}\")\n",
    "    print(f\"  LR: {current_lr:.2e}\")\n",
    "    \n",
    "    # Early stopping check\n",
    "    if early_stopping(val_loss, model):\n",
    "        print(f\"\\n‚èπÔ∏è  Early stopping triggered after {epoch+1} epochs\")\n",
    "        break\n",
    "    \n",
    "    # Save best model\n",
    "    if val_loss < best_val_loss_phase2:\n",
    "        best_val_loss_phase2 = val_loss\n",
    "        torch.save({\n",
    "            'epoch': phase1_epochs + epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'val_loss': val_loss,\n",
    "            'val_metrics': val_metrics,\n",
    "            'history': history,\n",
    "            'config': CONFIG\n",
    "        }, os.path.join(CONFIG['SAVE_PATH'], 'best_final_model.pth'))\n",
    "        print(f\"üíæ Best final model saved (Val Loss: {val_loss:.4f})\")\n",
    "    \n",
    "    phase2_epochs = epoch + 1\n",
    "\n",
    "print(f\"\\nüèÅ Phase 2 completed after {phase2_epochs} epochs\")\n",
    "print(f\"üèÜ Best Phase 2 validation loss: {best_val_loss_phase2:.4f}\")\n",
    "print(f\"üìä Total training epochs: {phase1_epochs + phase2_epochs}\")\n",
    "print(f\"üìà Final train accuracy: {history['train_acc'][-1]:.4f}\")\n",
    "print(f\"üìà Final validation accuracy: {history['val_acc'][-1]:.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"‚úÖ PHASE 2 TRAINING COMPLETED SUCCESSFULLY!\")\n",
    "print(\"=\"*70)\n",
    "print(\"üéâ TWO-PHASE TRAINING PIPELINE COMPLETED!\")\n",
    "print(\"=\"*70)"
]))

# Add the remaining cells to the notebook
notebook['cells'].extend(additional_cells)

# Save the updated notebook
with open('/workspace/diabetic_retinopathy_code_only.ipynb', 'w') as f:
    json.dump(notebook, f, indent=1)

print("Additional training cells added successfully!")