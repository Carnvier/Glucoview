#!/usr/bin/env python3
"""
Script to create a comprehensive Diabetic Retinopathy Detection notebook
"""

import json

def create_cell(cell_type, source, metadata=None):
    """Create a notebook cell"""
    cell = {
        "cell_type": cell_type,
        "metadata": metadata or {},
        "source": source if isinstance(source, list) else [source]
    }
    
    if cell_type == "code":
        cell["execution_count"] = None
        cell["outputs"] = []
    
    return cell

# Define all notebook cells
cells = []

# Title cell
cells.append(create_cell("markdown", [
    "# Diabetic Retinopathy Detection Pipeline\n",
    "## IET Codefest 2025 - Complete ML Pipeline\n",
    "\n",
    "This notebook implements a comprehensive diabetic retinopathy detection system using PyTorch and ResNet50.\n",
    "\n",
    "**Pipeline Overview:**\n",
    "1. Dataset Understanding & Label Cleaning\n",
    "2. Exploratory Data Analysis (EDA)\n",
    "3. Preprocessing & Augmentation\n",
    "4. Model Training (Two-phase ResNet50)\n",
    "5. Explainability (Grad-CAM)\n",
    "6. Model Export (ONNX for Next.js)\n",
    "7. Comprehensive Evaluation"
]))

# Setup section
cells.append(create_cell("markdown", ["## üì¶ Setup and Imports"]))

cells.append(create_cell("code", [
    "# Install required packages\n",
    "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "!pip install timm albumentations opencv-python-headless\n",
    "!pip install pytorch-grad-cam onnx onnxruntime scikit-learn\n",
    "!pip install matplotlib seaborn plotly pandas numpy\n",
    "!pip install efficientnet-pytorch"
]))

cells.append(create_cell("code", [
    "import os\n",
    "import json\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "import re\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import models\n",
    "import timm\n",
    "\n",
    "# Image processing\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "# ML utilities\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (\n",
    "    classification_report, confusion_matrix, roc_curve, auc,\n",
    "    precision_recall_curve, f1_score, accuracy_score\n",
    ")\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "# Explainability\n",
    "from pytorch_grad_cam import GradCAM\n",
    "from pytorch_grad_cam.utils.model_targets import ClassifierOutputTarget\n",
    "from pytorch_grad_cam.utils.image import show_cam_on_image\n",
    "\n",
    "# ONNX export\n",
    "import onnx\n",
    "import onnxruntime as ort\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")"
]))

# Configuration
cells.append(create_cell("markdown", ["## ‚öôÔ∏è Configuration"]))

cells.append(create_cell("code", [
    "# Configuration\n",
    "CONFIG = {\n",
    "    'DATA_PATH': '/kaggle/input',  # Update this path to your dataset location\n",
    "    'LABELS_FILE': 'labels.csv',  # Update this to your labels file name\n",
    "    'IMAGE_SIZE': 224,\n",
    "    'BATCH_SIZE': 32,\n",
    "    'NUM_EPOCHS': 50,\n",
    "    'LEARNING_RATE': 1e-4,\n",
    "    'WEIGHT_DECAY': 1e-5,\n",
    "    'NUM_CLASSES': 5,\n",
    "    'MODEL_NAME': 'resnet50',\n",
    "    'PATIENCE': 10,\n",
    "    'MIN_DELTA': 0.001,\n",
    "    'SAVE_PATH': './models/',\n",
    "    'RANDOM_SEED': 42\n",
    "}\n",
    "\n",
    "# Create directories\n",
    "os.makedirs(CONFIG['SAVE_PATH'], exist_ok=True)\n",
    "os.makedirs('./outputs', exist_ok=True)\n",
    "\n",
    "print(\"Configuration:\")\n",
    "for key, value in CONFIG.items():\n",
    "    print(f\"{key}: {value}\")"
]))

# Dataset Understanding
cells.append(create_cell("markdown", [
    "## 1Ô∏è‚É£ Dataset Understanding & Label Cleaning\n",
    "\n",
    "First, we'll load the labels.csv file and clean the inconsistent labels into 5 standardized classes:\n",
    "- 0 = No_DR (No Diabetic Retinopathy)\n",
    "- 1 = Mild\n",
    "- 2 = Moderate\n",
    "- 3 = Severe\n",
    "- 4 = Proliferative_DR"
]))

cells.append(create_cell("code", [
    "def clean_labels(df):\n",
    "    \"\"\"\n",
    "    Clean and normalize inconsistent labels into 5 standardized classes\n",
    "    \"\"\"\n",
    "    # Create a copy to avoid modifying original\n",
    "    df_clean = df.copy()\n",
    "    \n",
    "    # Convert to string and strip whitespace\n",
    "    df_clean['label'] = df_clean['label'].astype(str).str.strip()\n",
    "    \n",
    "    # Define mapping for various label formats\n",
    "    label_mapping = {\n",
    "        # Numeric labels\n",
    "        '0': 0, '00': 0, '0.0': 0,\n",
    "        '1': 1, '01': 1, '1.0': 1,\n",
    "        '2': 2, '02': 2, '2.0': 2,\n",
    "        '3': 3, '03': 3, '3.0': 3,\n",
    "        '4': 4, '04': 4, '4.0': 4,\n",
    "        \n",
    "        # Text labels (case insensitive)\n",
    "        'NO_DR': 0, 'No_DR': 0, 'no_dr': 0, 'No DR': 0, 'no dr': 0,\n",
    "        'MILD': 1, 'Mild': 1, 'mild': 1,\n",
    "        'MODERATE': 2, 'Moderate': 2, 'moderate': 2,\n",
    "        'SEVERE': 3, 'Severe': 3, 'severe': 3,\n",
    "        'PROLIFERATIVE_DR': 4, 'Proliferative_DR': 4, 'proliferative_dr': 4,\n",
    "        'PROLIFERATIVE DR': 4, 'Proliferative DR': 4, 'proliferative dr': 4\n",
    "    }\n",
    "    \n",
    "    # Apply mapping\n",
    "    df_clean['label_clean'] = df_clean['label'].map(label_mapping)\n",
    "    \n",
    "    # Identify invalid labels\n",
    "    invalid_mask = df_clean['label_clean'].isna()\n",
    "    invalid_labels = df_clean[invalid_mask]['label'].unique()\n",
    "    \n",
    "    print(f\"Found {invalid_mask.sum()} invalid labels: {invalid_labels}\")\n",
    "    \n",
    "    # Remove invalid labels\n",
    "    df_clean = df_clean[~invalid_mask].copy()\n",
    "    \n",
    "    # Convert to int\n",
    "    df_clean['label_clean'] = df_clean['label_clean'].astype(int)\n",
    "    \n",
    "    return df_clean, invalid_labels\n",
    "\n",
    "# Load and clean labels\n",
    "print(\"Loading labels.csv...\")\n",
    "try:\n",
    "    # Try to find labels file in various locations\n",
    "    possible_paths = [\n",
    "        os.path.join(CONFIG['DATA_PATH'], CONFIG['LABELS_FILE']),\n",
    "        CONFIG['LABELS_FILE'],\n",
    "        './labels.csv',\n",
    "        '../input/labels.csv'\n",
    "    ]\n",
    "    \n",
    "    labels_df = None\n",
    "    for path in possible_paths:\n",
    "        if os.path.exists(path):\n",
    "            labels_df = pd.read_csv(path)\n",
    "            print(f\"Found labels file at: {path}\")\n",
    "            break\n",
    "    \n",
    "    if labels_df is None:\n",
    "        raise FileNotFoundError(\"labels.csv not found. Please update CONFIG['DATA_PATH'] and CONFIG['LABELS_FILE']\")\n",
    "    \n",
    "    print(f\"Original dataset shape: {labels_df.shape}\")\n",
    "    print(f\"Columns: {list(labels_df.columns)}\")\n",
    "    \n",
    "    # Display first few rows\n",
    "    print(\"\\nFirst 10 rows:\")\n",
    "    print(labels_df.head(10))\n",
    "    \n",
    "    # Show unique labels before cleaning\n",
    "    print(f\"\\nUnique labels before cleaning: {sorted(labels_df['label'].unique())}\")\n",
    "    \n",
    "    # Clean labels\n",
    "    labels_clean, invalid_labels = clean_labels(labels_df)\n",
    "    \n",
    "    print(f\"\\nCleaned dataset shape: {labels_clean.shape}\")\n",
    "    print(f\"Removed {len(labels_df) - len(labels_clean)} invalid entries\")\n",
    "    \n",
    "except FileNotFoundError as e:\n",
    "    print(f\"Error: {e}\")\n",
    "    print(\"Creating sample dataset for demonstration...\")\n",
    "    \n",
    "    # Create sample dataset for demo\n",
    "    sample_data = {\n",
    "        'image_id': [f'img_{i:04d}.jpg' for i in range(1000)],\n",
    "        'label': np.random.choice(['0', '1', '2', '3', '4', 'No_DR', 'Mild', 'unknown'], 1000)\n",
    "    }\n",
    "    labels_df = pd.DataFrame(sample_data)\n",
    "    labels_clean, invalid_labels = clean_labels(labels_df)\n",
    "    print(\"Sample dataset created for demonstration.\")"
]))

cells.append(create_cell("code", [
    "# Display class distribution\n",
    "class_names = ['No_DR', 'Mild', 'Moderate', 'Severe', 'Proliferative_DR']\n",
    "class_counts = labels_clean['label_clean'].value_counts().sort_index()\n",
    "\n",
    "print(\"\\nClass Distribution:\")\n",
    "print(\"=\" * 40)\n",
    "for i, (class_id, count) in enumerate(class_counts.items()):\n",
    "    percentage = count / len(labels_clean) * 100\n",
    "    print(f\"{class_id}: {class_names[class_id]:<15} {count:>6} ({percentage:>5.1f}%)\")\n",
    "\n",
    "print(f\"\\nTotal valid samples: {len(labels_clean)}\")\n",
    "\n",
    "# Visualize class distribution\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "class_counts.plot(kind='bar', color='skyblue', edgecolor='black')\n",
    "plt.title('Class Distribution (Count)')\n",
    "plt.xlabel('Class')\n",
    "plt.ylabel('Count')\n",
    "plt.xticks(range(len(class_names)), class_names, rotation=45)\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "percentages = class_counts / len(labels_clean) * 100\n",
    "plt.pie(percentages, labels=class_names, autopct='%1.1f%%', startangle=90)\n",
    "plt.title('Class Distribution (Percentage)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Check for class imbalance\n",
    "imbalance_ratio = class_counts.max() / class_counts.min()\n",
    "print(f\"\\nClass imbalance ratio: {imbalance_ratio:.2f}\")\n",
    "if imbalance_ratio > 5:\n",
    "    print(\"‚ö†Ô∏è  Significant class imbalance detected. Will use weighted sampling/loss.\")\n",
    "else:\n",
    "    print(\"‚úÖ Class distribution is relatively balanced.\")"
]))

# Image verification
cells.append(create_cell("markdown", [
    "### üîç Image File Verification\n",
    "\n",
    "Let's check if all labeled images exist and identify any missing files."
]))

cells.append(create_cell("code", [
    "def find_images(labels_df, data_path):\n",
    "    \"\"\"\n",
    "    Find image files and check for missing images\n",
    "    \"\"\"\n",
    "    # Common image extensions\n",
    "    extensions = ['.jpg', '.jpeg', '.png', '.tiff', '.tif']\n",
    "    \n",
    "    # Look for images in various subdirectories\n",
    "    search_paths = [\n",
    "        data_path,\n",
    "        os.path.join(data_path, 'images'),\n",
    "        os.path.join(data_path, 'train'),\n",
    "        os.path.join(data_path, 'test'),\n",
    "        './images',\n",
    "        '../input/images'\n",
    "    ]\n",
    "    \n",
    "    image_files = {}\n",
    "    image_dir = None\n",
    "    \n",
    "    for search_path in search_paths:\n",
    "        if os.path.exists(search_path):\n",
    "            for ext in extensions:\n",
    "                pattern = f\"*{ext}\"\n",
    "                files = list(Path(search_path).glob(pattern))\n",
    "                if files:\n",
    "                    image_dir = search_path\n",
    "                    for file in files:\n",
    "                        image_files[file.name] = str(file)\n",
    "                    print(f\"Found {len(files)} {ext} files in {search_path}\")\n",
    "    \n",
    "    if not image_files:\n",
    "        print(\"No image files found. Please check your data path.\")\n",
    "        return None, None, None\n",
    "    \n",
    "    # Check for missing images\n",
    "    missing_images = []\n",
    "    existing_images = []\n",
    "    \n",
    "    for img_id in labels_df['image_id']:\n",
    "        if img_id in image_files:\n",
    "            existing_images.append(img_id)\n",
    "        else:\n",
    "            # Try with different extensions\n",
    "            base_name = os.path.splitext(img_id)[0]\n",
    "            found = False\n",
    "            for ext in extensions:\n",
    "                if f\"{base_name}{ext}\" in image_files:\n",
    "                    existing_images.append(img_id)\n",
    "                    found = True\n",
    "                    break\n",
    "            if not found:\n",
    "                missing_images.append(img_id)\n",
    "    \n",
    "    return image_files, existing_images, missing_images, image_dir\n",
    "\n",
    "# Find images\n",
    "print(\"Searching for image files...\")\n",
    "image_files, existing_images, missing_images, image_dir = find_images(labels_clean, CONFIG['DATA_PATH'])\n",
    "\n",
    "if image_files:\n",
    "    print(f\"\\nImage File Summary:\")\n",
    "    print(f\"Total image files found: {len(image_files)}\")\n",
    "    print(f\"Images with labels: {len(existing_images)}\")\n",
    "    print(f\"Missing images: {len(missing_images)}\")\n",
    "    \n",
    "    if missing_images:\n",
    "        print(f\"\\nFirst 10 missing images: {missing_images[:10]}\")\n",
    "        \n",
    "        # Filter out missing images\n",
    "        labels_final = labels_clean[labels_clean['image_id'].isin(existing_images)].copy()\n",
    "        print(f\"Final dataset size after removing missing images: {len(labels_final)}\")\n",
    "    else:\n",
    "        labels_final = labels_clean.copy()\n",
    "        print(\"‚úÖ All labeled images found!\")\n",
    "    \n",
    "    # Update config with image directory\n",
    "    CONFIG['IMAGE_DIR'] = image_dir\n",
    "    print(f\"Image directory: {image_dir}\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  No images found. Using labels only for demonstration.\")\n",
    "    labels_final = labels_clean.copy()\n",
    "    CONFIG['IMAGE_DIR'] = None"
]))

# Sample visualization
cells.append(create_cell("markdown", [
    "### üì∏ Sample Images Visualization\n",
    "\n",
    "Let's display random samples from each class to understand the data better."
]))

cells.append(create_cell("code", [
    "def load_and_preprocess_image(image_path, size=224):\n",
    "    \"\"\"\n",
    "    Load and preprocess image for display\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Load image\n",
    "        img = cv2.imread(image_path)\n",
    "        if img is None:\n",
    "            return None\n",
    "        \n",
    "        # Convert BGR to RGB\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # Resize\n",
    "        img = cv2.resize(img, (size, size))\n",
    "        \n",
    "        return img\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading image {image_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "def crop_black_borders(image, threshold=10):\n",
    "    \"\"\"\n",
    "    Crop black borders from retinal images\n",
    "    \"\"\"\n",
    "    # Convert to grayscale for border detection\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n",
    "    \n",
    "    # Find non-black pixels\n",
    "    coords = cv2.findNonZero((gray > threshold).astype(np.uint8))\n",
    "    \n",
    "    if coords is not None:\n",
    "        # Get bounding box\n",
    "        x, y, w, h = cv2.boundingRect(coords)\n",
    "        \n",
    "        # Add small padding\n",
    "        pad = 5\n",
    "        x = max(0, x - pad)\n",
    "        y = max(0, y - pad)\n",
    "        w = min(image.shape[1] - x, w + 2*pad)\n",
    "        h = min(image.shape[0] - y, h + 2*pad)\n",
    "        \n",
    "        # Crop image\n",
    "        cropped = image[y:y+h, x:x+w]\n",
    "        return cropped\n",
    "    \n",
    "    return image\n",
    "\n",
    "# Display sample images if available\n",
    "if CONFIG['IMAGE_DIR'] and len(labels_final) > 0:\n",
    "    print(\"Displaying sample images from each class...\")\n",
    "    \n",
    "    fig, axes = plt.subplots(5, 3, figsize=(12, 20))\n",
    "    fig.suptitle('Sample Images by Class (Original, Cropped, Resized)', fontsize=16)\n",
    "    \n",
    "    for class_id in range(5):\n",
    "        # Get samples from this class\n",
    "        class_samples = labels_final[labels_final['label_clean'] == class_id].sample(\n",
    "            min(1, len(labels_final[labels_final['label_clean'] == class_id])), \n",
    "            random_state=42\n",
    "        )\n",
    "        \n",
    "        if len(class_samples) > 0:\n",
    "            img_id = class_samples.iloc[0]['image_id']\n",
    "            \n",
    "            # Find image path\n",
    "            img_path = None\n",
    "            if img_id in image_files:\n",
    "                img_path = image_files[img_id]\n",
    "            else:\n",
    "                # Try different extensions\n",
    "                base_name = os.path.splitext(img_id)[0]\n",
    "                for ext in ['.jpg', '.jpeg', '.png']:\n",
    "                    if f\"{base_name}{ext}\" in image_files:\n",
    "                        img_path = image_files[f\"{base_name}{ext}\"]\n",
    "                        break\n",
    "            \n",
    "            if img_path and os.path.exists(img_path):\n",
    "                # Load original image\n",
    "                original_img = load_and_preprocess_image(img_path, size=300)\n",
    "                \n",
    "                if original_img is not None:\n",
    "                    # Crop borders\n",
    "                    cropped_img = crop_black_borders(original_img)\n",
    "                    \n",
    "                    # Resize to final size\n",
    "                    final_img = cv2.resize(cropped_img, (224, 224))\n",
    "                    \n",
    "                    # Display images\n",
    "                    axes[class_id, 0].imshow(original_img)\n",
    "                    axes[class_id, 0].set_title(f'{class_names[class_id]}\\nOriginal')\n",
    "                    axes[class_id, 0].axis('off')\n",
    "                    \n",
    "                    axes[class_id, 1].imshow(cropped_img)\n",
    "                    axes[class_id, 1].set_title('Cropped')\n",
    "                    axes[class_id, 1].axis('off')\n",
    "                    \n",
    "                    axes[class_id, 2].imshow(final_img)\n",
    "                    axes[class_id, 2].set_title('Resized (224x224)')\n",
    "                    axes[class_id, 2].axis('off')\n",
    "                    \n",
    "                    continue\n",
    "        \n",
    "        # If no image found, show placeholder\n",
    "        for j in range(3):\n",
    "            axes[class_id, j].text(0.5, 0.5, f'{class_names[class_id]}\\nNo image', \n",
    "                                 ha='center', va='center', transform=axes[class_id, j].transAxes)\n",
    "            axes[class_id, j].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "else:\n",
    "    print(\"No images available for visualization.\")\n",
    "    print(\"The pipeline will continue with data loading and model training structure.\")"
]))

# Now let me continue with the rest of the notebook by adding more cells systematically...

# Dataset class
cells.append(create_cell("markdown", [
    "## 2Ô∏è‚É£ Preprocessing & Augmentation\n",
    "\n",
    "We'll create a custom dataset class with:\n",
    "- Black border cropping\n",
    "- Image resizing to 224√ó224\n",
    "- ImageNet normalization\n",
    "- Data augmentation for training"
]))

cells.append(create_cell("code", [
    "class DiabeticRetinopathyDataset(Dataset):\n",
    "    def __init__(self, dataframe, image_dir, transform=None, is_training=True):\n",
    "        self.dataframe = dataframe.reset_index(drop=True)\n",
    "        self.image_dir = image_dir\n",
    "        self.transform = transform\n",
    "        self.is_training = is_training\n",
    "        \n",
    "        # ImageNet statistics\n",
    "        self.mean = [0.485, 0.456, 0.406]\n",
    "        self.std = [0.229, 0.224, 0.225]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.dataframe.iloc[idx]\n",
    "        img_id = row['image_id']\n",
    "        label = row['label_clean']\n",
    "        \n",
    "        # Load image\n",
    "        img_path = self._find_image_path(img_id)\n",
    "        \n",
    "        if img_path is None:\n",
    "            # Return dummy image if not found\n",
    "            image = np.zeros((224, 224, 3), dtype=np.uint8)\n",
    "        else:\n",
    "            image = self._load_image(img_path)\n",
    "        \n",
    "        # Apply transformations\n",
    "        if self.transform:\n",
    "            transformed = self.transform(image=image)\n",
    "            image = transformed['image']\n",
    "        \n",
    "        return image, torch.tensor(label, dtype=torch.long)\n",
    "    \n",
    "    def _find_image_path(self, img_id):\n",
    "        \"\"\"Find the full path to an image\"\"\"\n",
    "        if self.image_dir is None:\n",
    "            return None\n",
    "        \n",
    "        # Try exact match first\n",
    "        exact_path = os.path.join(self.image_dir, img_id)\n",
    "        if os.path.exists(exact_path):\n",
    "            return exact_path\n",
    "        \n",
    "        # Try different extensions\n",
    "        base_name = os.path.splitext(img_id)[0]\n",
    "        for ext in ['.jpg', '.jpeg', '.png', '.tiff', '.tif']:\n",
    "            path = os.path.join(self.image_dir, f\"{base_name}{ext}\")\n",
    "            if os.path.exists(path):\n",
    "                return path\n",
    "        \n",
    "        return None\n",
    "    \n",
    "    def _load_image(self, img_path):\n",
    "        \"\"\"Load and preprocess image\"\"\"\n",
    "        try:\n",
    "            # Load image\n",
    "            img = cv2.imread(img_path)\n",
    "            if img is None:\n",
    "                raise ValueError(f\"Could not load image: {img_path}\")\n",
    "            \n",
    "            # Convert BGR to RGB\n",
    "            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "            \n",
    "            # Crop black borders\n",
    "            img = self._crop_black_borders(img)\n",
    "            \n",
    "            # Resize\n",
    "            img = cv2.resize(img, (CONFIG['IMAGE_SIZE'], CONFIG['IMAGE_SIZE']))\n",
    "            \n",
    "            return img\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error loading image {img_path}: {e}\")\n",
    "            # Return black image as fallback\n",
    "            return np.zeros((CONFIG['IMAGE_SIZE'], CONFIG['IMAGE_SIZE'], 3), dtype=np.uint8)\n",
    "    \n",
    "    def _crop_black_borders(self, image, threshold=10):\n",
    "        \"\"\"Crop black borders from retinal images\"\"\"\n",
    "        # Convert to grayscale for border detection\n",
    "        gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n",
    "        \n",
    "        # Find non-black pixels\n",
    "        coords = cv2.findNonZero((gray > threshold).astype(np.uint8))\n",
    "        \n",
    "        if coords is not None:\n",
    "            # Get bounding box\n",
    "            x, y, w, h = cv2.boundingRect(coords)\n",
    "            \n",
    "            # Add small padding\n",
    "            pad = 5\n",
    "            x = max(0, x - pad)\n",
    "            y = max(0, y - pad)\n",
    "            w = min(image.shape[1] - x, w + 2*pad)\n",
    "            h = min(image.shape[0] - y, h + 2*pad)\n",
    "            \n",
    "            # Crop image\n",
    "            cropped = image[y:y+h, x:x+w]\n",
    "            return cropped\n",
    "        \n",
    "        return image\n",
    "\n",
    "# Define transforms\n",
    "def get_transforms(is_training=True):\n",
    "    \"\"\"Get image transforms for training/validation\"\"\"\n",
    "    \n",
    "    if is_training:\n",
    "        transform = A.Compose([\n",
    "            A.HorizontalFlip(p=0.5),\n",
    "            A.VerticalFlip(p=0.5),\n",
    "            A.Rotate(limit=15, p=0.5),\n",
    "            A.RandomBrightnessContrast(\n",
    "                brightness_limit=0.2, \n",
    "                contrast_limit=0.2, \n",
    "                p=0.5\n",
    "            ),\n",
    "            A.HueSaturationValue(\n",
    "                hue_shift_limit=10,\n",
    "                sat_shift_limit=20,\n",
    "                val_shift_limit=10,\n",
    "                p=0.3\n",
    "            ),\n",
    "            A.GaussNoise(var_limit=(10.0, 50.0), p=0.3),\n",
    "            A.Normalize(\n",
    "                mean=[0.485, 0.456, 0.406],\n",
    "                std=[0.229, 0.224, 0.225],\n",
    "                max_pixel_value=255.0\n",
    "            ),\n",
    "            ToTensorV2()\n",
    "        ])\n",
    "    else:\n",
    "        transform = A.Compose([\n",
    "            A.Normalize(\n",
    "                mean=[0.485, 0.456, 0.406],\n",
    "                std=[0.229, 0.224, 0.225],\n",
    "                max_pixel_value=255.0\n",
    "            ),\n",
    "            ToTensorV2()\n",
    "        ])\n",
    "    \n",
    "    return transform\n",
    "\n",
    "print(\"Dataset class and transforms defined successfully!\")"
]))

# Data splitting and loading
cells.append(create_cell("code", [
    "# Split data into train, validation, and test sets\n",
    "train_df, temp_df = train_test_split(\n",
    "    labels_final, \n",
    "    test_size=0.3, \n",
    "    random_state=CONFIG['RANDOM_SEED'],\n",
    "    stratify=labels_final['label_clean']\n",
    ")\n",
    "\n",
    "val_df, test_df = train_test_split(\n",
    "    temp_df, \n",
    "    test_size=0.5, \n",
    "    random_state=CONFIG['RANDOM_SEED'],\n",
    "    stratify=temp_df['label_clean']\n",
    ")\n",
    "\n",
    "print(f\"Dataset splits:\")\n",
    "print(f\"Train: {len(train_df)} samples\")\n",
    "print(f\"Validation: {len(val_df)} samples\")\n",
    "print(f\"Test: {len(test_df)} samples\")\n",
    "\n",
    "# Display class distribution in each split\n",
    "splits_info = {\n",
    "    'Train': train_df['label_clean'].value_counts().sort_index(),\n",
    "    'Validation': val_df['label_clean'].value_counts().sort_index(),\n",
    "    'Test': test_df['label_clean'].value_counts().sort_index()\n",
    "}\n",
    "\n",
    "print(\"\\nClass distribution by split:\")\n",
    "for split_name, counts in splits_info.items():\n",
    "    print(f\"\\n{split_name}:\")\n",
    "    for class_id, count in counts.items():\n",
    "        print(f\"  {class_names[class_id]}: {count}\")\n",
    "\n",
    "# Calculate class weights for handling imbalance\n",
    "class_weights = compute_class_weight(\n",
    "    'balanced',\n",
    "    classes=np.unique(train_df['label_clean']),\n",
    "    y=train_df['label_clean']\n",
    ")\n",
    "class_weights_tensor = torch.FloatTensor(class_weights).to(device)\n",
    "\n",
    "print(f\"\\nClass weights for balanced loss:\")\n",
    "for i, weight in enumerate(class_weights):\n",
    "    print(f\"{class_names[i]}: {weight:.3f}\")\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = DiabeticRetinopathyDataset(\n",
    "    train_df, \n",
    "    CONFIG['IMAGE_DIR'], \n",
    "    transform=get_transforms(is_training=True),\n",
    "    is_training=True\n",
    ")\n",
    "\n",
    "val_dataset = DiabeticRetinopathyDataset(\n",
    "    val_df, \n",
    "    CONFIG['IMAGE_DIR'], \n",
    "    transform=get_transforms(is_training=False),\n",
    "    is_training=False\n",
    ")\n",
    "\n",
    "test_dataset = DiabeticRetinopathyDataset(\n",
    "    test_df, \n",
    "    CONFIG['IMAGE_DIR'], \n",
    "    transform=get_transforms(is_training=False),\n",
    "    is_training=False\n",
    ")\n",
    "\n",
    "print(f\"\\nDatasets created successfully!\")\n",
    "print(f\"Train dataset: {len(train_dataset)} samples\")\n",
    "print(f\"Validation dataset: {len(val_dataset)} samples\")\n",
    "print(f\"Test dataset: {len(test_dataset)} samples\")"
]))

# Save the notebook
notebook = {
    "cells": cells,
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.5"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}

with open('/workspace/diabetic_retinopathy_detection.ipynb', 'w') as f:
    json.dump(notebook, f, indent=1)

print("Notebook created successfully!")