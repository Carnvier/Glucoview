#!/usr/bin/env python3
"""
Script to create TensorFlow/Keras version of diabetic retinopathy detection notebook
"""

import json

def create_cell(cell_type, source, metadata=None):
    """Create a notebook cell"""
    cell = {
        "cell_type": cell_type,
        "metadata": metadata or {},
        "source": source if isinstance(source, list) else [source]
    }
    
    if cell_type == "code":
        cell["execution_count"] = None
        cell["outputs"] = []
    
    return cell

# Define all notebook cells using TensorFlow
cells = []

# Title and setup
cells.append(create_cell("code", [
    "# ============================================================================\n",
    "# DIABETIC RETINOPATHY DETECTION PIPELINE - TENSORFLOW VERSION\n",
    "# ============================================================================\n",
    "# Complete ML Pipeline using TensorFlow/Keras and ResNet50\n",
    "# IET Codefest 2025 - Optimized for TensorFlow deployment\n",
    "#\n",
    "# Pipeline Overview:\n",
    "# 1. Dataset Understanding & Label Cleaning\n",
    "# 2. Exploratory Data Analysis (EDA)\n",
    "# 3. Preprocessing & Augmentation (tf.data)\n",
    "# 4. Model Training (Two-phase ResNet50)\n",
    "# 5. Explainability (GradCAM with tf-explain)\n",
    "# 6. Model Export (SavedModel & TensorFlow.js)\n",
    "# 7. Comprehensive Evaluation\n",
    "# ============================================================================\n",
    "\n",
    "print(\"üöÄ Starting TensorFlow Diabetic Retinopathy Detection Pipeline\")\n",
    "print(\"üìã IET Codefest 2025 - TensorFlow/Keras Implementation\")\n",
    "print(\"‚ö° Optimized for production deployment\")"
]))

# Install packages
cells.append(create_cell("code", [
    "# ============================================================================\n",
    "# TENSORFLOW PACKAGE INSTALLATION\n",
    "# ============================================================================\n",
    "# Install TensorFlow and related packages for the complete pipeline\n",
    "\n",
    "!pip install tensorflow>=2.13.0\n",
    "!pip install tensorflow-addons\n",
    "!pip install tf-explain  # For GradCAM explainability\n",
    "!pip install opencv-python-headless\n",
    "!pip install matplotlib seaborn plotly pandas numpy\n",
    "!pip install scikit-learn\n",
    "!pip install Pillow\n",
    "!pip install tensorboard\n",
    "\n",
    "# Optional: TensorFlow.js conversion tools\n",
    "!pip install tensorflowjs\n",
    "\n",
    "print(\"‚úÖ All TensorFlow packages installed successfully!\")"
]))

# Imports
cells.append(create_cell("code", [
    "# ============================================================================\n",
    "# TENSORFLOW IMPORTS AND SETUP\n",
    "# ============================================================================\n",
    "# Import all necessary libraries for TensorFlow-based pipeline\n",
    "\n",
    "import os\n",
    "import json\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "import re\n",
    "\n",
    "# TensorFlow imports\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, models, optimizers, callbacks\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import tensorflow_addons as tfa\n",
    "\n",
    "# Image processing\n",
    "import cv2\n",
    "from PIL import Image\n",
    "\n",
    "# ML utilities\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (\n",
    "    classification_report, confusion_matrix, roc_curve, auc,\n",
    "    precision_recall_curve, f1_score, accuracy_score\n",
    ")\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "# Explainability with tf-explain\n",
    "try:\n",
    "    from tf_explain.core.grad_cam import GradCAM\n",
    "    gradcam_available = True\n",
    "except ImportError:\n",
    "    print(\"‚ö†Ô∏è  tf-explain not available. GradCAM will be skipped.\")\n",
    "    gradcam_available = False\n",
    "\n",
    "# Configure environment\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Configure TensorFlow\n",
    "print(f\"üîß TensorFlow version: {tf.__version__}\")\n",
    "print(f\"üîß Keras version: {keras.__version__}\")\n",
    "\n",
    "# Check for GPU\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        # Enable memory growth to avoid allocating all GPU memory\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        print(f\"üéÆ GPU available: {len(gpus)} device(s)\")\n",
    "        print(f\"üéÆ GPU details: {gpus[0]}\")\n",
    "    except RuntimeError as e:\n",
    "        print(f\"GPU setup error: {e}\")\nelse:\n",
    "    print(\"üíª Using CPU for training\")\n",
    "\n",
    "print(\"‚úÖ All TensorFlow imports loaded successfully!\")"
]))

# Configuration
cells.append(create_cell("code", [
    "# ============================================================================\n",
    "# TENSORFLOW PIPELINE CONFIGURATION\n",
    "# ============================================================================\n",
    "# Main configuration optimized for TensorFlow/Keras training\n",
    "\n",
    "CONFIG = {\n",
    "    # Data paths - UPDATE THESE FOR YOUR DATASET\n",
    "    'DATA_PATH': '/kaggle/input',          # Update this path to your dataset location\n",
    "    'LABELS_FILE': 'labels.csv',           # Update this to your labels file name\n",
    "    \n",
    "    # Model parameters\n",
    "    'IMAGE_SIZE': 224,                     # Input image size (224x224)\n",
    "    'BATCH_SIZE': 32,                      # Training batch size\n",
    "    'NUM_EPOCHS': 50,                      # Maximum training epochs\n",
    "    'LEARNING_RATE': 1e-4,                 # Initial learning rate\n",
    "    'NUM_CLASSES': 5,                      # Number of classification classes\n",
    "    'MODEL_NAME': 'resnet50',              # Model architecture\n",
    "    \n",
    "    # Training parameters\n",
    "    'PATIENCE': 10,                        # Early stopping patience\n",
    "    'MIN_DELTA': 0.001,                    # Minimum improvement for early stopping\n",
    "    'VALIDATION_SPLIT': 0.15,              # Validation split ratio\n",
    "    'TEST_SPLIT': 0.15,                    # Test split ratio\n",
    "    \n",
    "    # TensorFlow specific\n",
    "    'MIXED_PRECISION': True,               # Use mixed precision training\n",
    "    'PREFETCH_BUFFER': tf.data.AUTOTUNE,   # Dataset prefetch buffer\n",
    "    'CACHE_DATASET': True,                 # Cache dataset in memory\n",
    "    \n",
    "    # Paths\n",
    "    'SAVE_PATH': './models/',              # Model save directory\n",
    "    'TENSORBOARD_PATH': './logs/',         # TensorBoard logs\n",
    "    'RANDOM_SEED': 42                      # Random seed for reproducibility\n",
    "}\n",
    "\n",
    "# Enable mixed precision if supported\n",
    "if CONFIG['MIXED_PRECISION']:\n",
    "    try:\n",
    "        policy = tf.keras.mixed_precision.Policy('mixed_float16')\n",
    "        tf.keras.mixed_precision.set_global_policy(policy)\n",
    "        print(\"‚úÖ Mixed precision training enabled\")\n",
    "    except:\n",
    "        print(\"‚ö†Ô∏è  Mixed precision not supported, using float32\")\n",
    "        CONFIG['MIXED_PRECISION'] = False\n",
    "\n",
    "# Create necessary directories\n",
    "os.makedirs(CONFIG['SAVE_PATH'], exist_ok=True)\n",
    "os.makedirs('./outputs', exist_ok=True)\n",
    "os.makedirs(CONFIG['TENSORBOARD_PATH'], exist_ok=True)\n",
    "\n",
    "# Display configuration\n",
    "print(\"‚öôÔ∏è  TensorFlow Pipeline Configuration:\")\n",
    "print(\"=\" * 50)\n",
    "for key, value in CONFIG.items():\n",
    "    print(f\"{key:<20}: {value}\")\n",
    "print(\"=\" * 50)\n",
    "print(\"‚úÖ TensorFlow configuration loaded successfully!\")"
]))

# Label cleaning (same as PyTorch version)
cells.append(create_cell("code", [
    "# ============================================================================\n",
    "# LABEL CLEANING AND NORMALIZATION\n",
    "# ============================================================================\n",
    "# Clean and normalize inconsistent labels (same logic as PyTorch version)\n",
    "\n",
    "def clean_labels(df):\n",
    "    \"\"\"\n",
    "    Clean and normalize inconsistent labels into 5 standardized classes\n",
    "    0 = No_DR, 1 = Mild, 2 = Moderate, 3 = Severe, 4 = Proliferative_DR\n",
    "    \"\"\"\n",
    "    print(\"üßπ Starting label cleaning process...\")\n",
    "    \n",
    "    df_clean = df.copy()\n",
    "    df_clean['label'] = df_clean['label'].astype(str).str.strip()\n",
    "    \n",
    "    # Comprehensive label mapping\n",
    "    label_mapping = {\n",
    "        # Numeric labels\n",
    "        '0': 0, '00': 0, '0.0': 0,\n",
    "        '1': 1, '01': 1, '1.0': 1,\n",
    "        '2': 2, '02': 2, '2.0': 2,\n",
    "        '3': 3, '03': 3, '3.0': 3,\n",
    "        '4': 4, '04': 4, '4.0': 4,\n",
    "        \n",
    "        # Text labels (case insensitive)\n",
    "        'NO_DR': 0, 'No_DR': 0, 'no_dr': 0, 'No DR': 0, 'no dr': 0, 'NO DR': 0,\n",
    "        'MILD': 1, 'Mild': 1, 'mild': 1,\n",
    "        'MODERATE': 2, 'Moderate': 2, 'moderate': 2,\n",
    "        'SEVERE': 3, 'Severe': 3, 'severe': 3,\n",
    "        'PROLIFERATIVE_DR': 4, 'Proliferative_DR': 4, 'proliferative_dr': 4,\n",
    "        'PROLIFERATIVE DR': 4, 'Proliferative DR': 4, 'proliferative dr': 4\n",
    "    }\n",
    "    \n",
    "    df_clean['label_clean'] = df_clean['label'].map(label_mapping)\n",
    "    invalid_mask = df_clean['label_clean'].isna()\n",
    "    invalid_labels = df_clean[invalid_mask]['label'].unique()\n",
    "    \n",
    "    print(f\"üìä Found {invalid_mask.sum()} invalid labels: {list(invalid_labels)}\")\n",
    "    \n",
    "    df_clean = df_clean[~invalid_mask].copy()\n",
    "    df_clean['label_clean'] = df_clean['label_clean'].astype(int)\n",
    "    \n",
    "    print(f\"‚úÖ Label cleaning completed: {len(df_clean)} valid samples\")\n",
    "    return df_clean, invalid_labels\n",
    "\n",
    "# Define class names\n",
    "class_names = ['No_DR', 'Mild', 'Moderate', 'Severe', 'Proliferative_DR']\n",
    "print(f\"üìù Class mapping: {dict(enumerate(class_names))}\")"
]))

# Data loading (same logic)
cells.append(create_cell("code", [
    "# ============================================================================\n",
    "# DATA LOADING AND CLEANING\n",
    "# ============================================================================\n",
    "# Load and clean the dataset (same as PyTorch version)\n",
    "\n",
    "print(\"üìÇ Loading labels.csv...\")\n",
    "try:\n",
    "    # Try to find labels file in various locations\n",
    "    possible_paths = [\n",
    "        os.path.join(CONFIG['DATA_PATH'], CONFIG['LABELS_FILE']),\n",
    "        CONFIG['LABELS_FILE'],\n",
    "        './labels.csv',\n",
    "        '../input/labels.csv',\n",
    "        './labels (1).csv'\n",
    "    ]\n",
    "    \n",
    "    labels_df = None\n",
    "    for path in possible_paths:\n",
    "        if os.path.exists(path):\n",
    "            labels_df = pd.read_csv(path)\n",
    "            print(f\"‚úÖ Found labels file at: {path}\")\n",
    "            break\n",
    "    \n",
    "    if labels_df is None:\n",
    "        raise FileNotFoundError(\"labels.csv not found\")\n",
    "    \n",
    "    print(f\"üìä Original dataset shape: {labels_df.shape}\")\n",
    "    print(f\"üìã Columns: {list(labels_df.columns)}\")\n",
    "    \n",
    "    # Clean labels\n",
    "    labels_clean, invalid_labels = clean_labels(labels_df)\n",
    "    \nexcept FileNotFoundError:\n",
    "    print(\"üîß Creating sample dataset for demonstration...\")\n",
    "    np.random.seed(42)\n",
    "    sample_data = {\n",
    "        'image_id': [f'img_{i:04d}.jpg' for i in range(1000)],\n",
    "        'label': np.random.choice(['0', '1', '2', '3', '4', 'No_DR', 'Mild', 'unknown'], 1000)\n",
    "    }\n",
    "    labels_df = pd.DataFrame(sample_data)\n",
    "    labels_clean, invalid_labels = clean_labels(labels_df)\n",
    "    print(\"‚úÖ Sample dataset created.\")\n",
    "\n",
    "print(f\"\\nüìà Final dataset: {len(labels_clean)} samples\")\n",
    "print(\"‚úÖ Data loading completed!\")"
]))

# TensorFlow data pipeline
cells.append(create_cell("code", [
    "# ============================================================================\n",
    "# TENSORFLOW DATA PIPELINE\n",
    "# ============================================================================\n",
    "# Create efficient tf.data pipeline for image loading and preprocessing\n",
    "\n",
    "def find_images(labels_df, data_path):\n",
    "    \"\"\"Find image files and return mapping\"\"\"\n",
    "    print(\"üîç Searching for image files...\")\n",
    "    \n",
    "    extensions = ['.jpg', '.jpeg', '.png', '.tiff', '.tif']\n",
    "    search_paths = [data_path, os.path.join(data_path, 'images'), './images', './']\n",
    "    \n",
    "    image_files = {}\n",
    "    image_dir = None\n",
    "    \n",
    "    for search_path in search_paths:\n",
    "        if os.path.exists(search_path):\n",
    "            for ext in extensions:\n",
    "                files = list(Path(search_path).glob(f\"*{ext}\"))\n",
    "                if files:\n",
    "                    if image_dir is None:\n",
    "                        image_dir = search_path\n",
    "                    for file in files:\n",
    "                        image_files[file.name] = str(file)\n",
    "                    print(f\"üìÅ Found {len(files)} {ext} files in {search_path}\")\n",
    "    \n",
    "    return image_files, image_dir\n",
    "\n",
    "def crop_black_borders_tf(image):\n",
    "    \"\"\"\n",
    "    TensorFlow function to crop black borders from retinal images\n",
    "    \"\"\"\n",
    "    # Convert to grayscale\n",
    "    gray = tf.image.rgb_to_grayscale(image)\n",
    "    \n",
    "    # Create mask for non-black pixels\n",
    "    mask = tf.greater(gray, 10.0/255.0)\n",
    "    \n",
    "    # Find bounding box\n",
    "    mask_indices = tf.where(mask[:, :, 0])\n",
    "    \n",
    "    if tf.size(mask_indices) > 0:\n",
    "        y_min = tf.reduce_min(mask_indices[:, 0])\n",
    "        y_max = tf.reduce_max(mask_indices[:, 0])\n",
    "        x_min = tf.reduce_min(mask_indices[:, 1])\n",
    "        x_max = tf.reduce_max(mask_indices[:, 1])\n",
    "        \n",
    "        # Add padding\n",
    "        pad = 5\n",
    "        height, width = tf.shape(image)[0], tf.shape(image)[1]\n",
    "        y_min = tf.maximum(0, y_min - pad)\n",
    "        x_min = tf.maximum(0, x_min - pad)\n",
    "        y_max = tf.minimum(height, y_max + pad)\n",
    "        x_max = tf.minimum(width, x_max + pad)\n",
    "        \n",
    "        # Crop image\n",
    "        cropped = image[y_min:y_max, x_min:x_max]\n",
    "        return cropped\n",
    "    \n",
    "    return image\n",
    "\n",
    "def preprocess_image(image_path, label, is_training=True):\n",
    "    \"\"\"\n",
    "    TensorFlow function to preprocess images\n",
    "    \"\"\"\n",
    "    # Load image\n",
    "    image = tf.io.read_file(image_path)\n",
    "    image = tf.image.decode_image(image, channels=3, expand_animations=False)\n",
    "    image = tf.cast(image, tf.float32)\n",
    "    \n",
    "    # Crop black borders\n",
    "    image = crop_black_borders_tf(image)\n",
    "    \n",
    "    # Resize\n",
    "    image = tf.image.resize(image, [CONFIG['IMAGE_SIZE'], CONFIG['IMAGE_SIZE']])\n",
    "    \n",
    "    # Data augmentation for training\n",
    "    if is_training:\n",
    "        image = tf.image.random_flip_left_right(image)\n",
    "        image = tf.image.random_flip_up_down(image)\n",
    "        image = tf.image.random_brightness(image, 0.2)\n",
    "        image = tf.image.random_contrast(image, 0.8, 1.2)\n",
    "        image = tf.image.random_hue(image, 0.1)\n",
    "        image = tf.image.random_saturation(image, 0.8, 1.2)\n",
    "    \n",
    "    # Normalize to [0, 1]\n",
    "    image = image / 255.0\n",
    "    \n",
    "    # ImageNet normalization\n",
    "    mean = tf.constant([0.485, 0.456, 0.406])\n",
    "    std = tf.constant([0.229, 0.224, 0.225])\n",
    "    image = (image - mean) / std\n",
    "    \n",
    "    return image, label\n",
    "\n",
    "# Find images\n",
    "image_files, image_dir = find_images(labels_clean, CONFIG['DATA_PATH'])\n",
    "CONFIG['IMAGE_DIR'] = image_dir\n",
    "\n",
    "if image_files:\n",
    "    print(f\"‚úÖ Found {len(image_files)} image files\")\n",
    "    # Filter labels to only include images that exist\n",
    "    labels_final = labels_clean[labels_clean['image_id'].isin(image_files.keys())].copy()\n",
    "    print(f\"üìä Final dataset: {len(labels_final)} samples with images\")\nelse:\n",
    "    print(\"‚ö†Ô∏è  No images found. Using dummy data.\")\n",
    "    labels_final = labels_clean.copy()\n",
    "\n",
    "print(\"‚úÖ TensorFlow data pipeline setup completed!\")"
]))

# Continue building the TensorFlow notebook...
# I'll add more cells in the next part to keep response manageable

# Save the first part
notebook = {\n    "cells": cells,\n    "metadata": {\n        "kernelspec": {\n            "display_name": "Python 3",\n            "language": "python",\n            "name": "python3"\n        },\n        "language_info": {\n            "codemirror_mode": {\n                "name": "ipython",\n                "version": 3\n            },\n            "file_extension": ".py",\n            "name": "python",\n            "nbconvert_exporter": "python",\n            "pygments_lexer": "ipython3",\n            "version": "3.8.5"\n        }\n    },\n    "nbformat": 4,\n    "nbformat_minor": 4\n}\n\nwith open('/workspace/diabetic_retinopathy_tensorflow.ipynb', 'w') as f:\n    json.dump(notebook, f, indent=1)\n\nprint("TensorFlow notebook (Part 1) created successfully!")